---
title: "Predicting salaries of WNBA players"
author: Caban
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 3
    toc_float: yes
urlcolor: blue
params:
  log_transform: true
  evaluate_chunks: false
  arrange_by:
    label: "Performance metric to arrange the models by"
    value: "Rsquared"
    input: select
    choices: ["RMSE", "MAE", "Rsquared"]
  
---


```{r setup, include=FALSE}

# Set knitr options
knitr::opts_chunk$set(
  echo = T, 
  fig.width=8, 
  # fig.height=4, 
  warning = F, 
  dpi = 300,
  message = F,
  comment = NA
  )





if(!require('pacman')) {install.packages('pacman')}

# Load all libraries
pacman::p_load(modelr, xgboost, DT, readxl,leaps,furrr, rstatix, broom, bestglm, glmnet, leaps, car,  pROC, caret, tree, rpart, randomForest, rpart.plot, doParallel, tidyverse)


# Set up parallel backend
num_cores <- detectCores() - 1

```






```{r}

# create functions and general ggplot vectors to streamline code and make it more reproducible




# Function to control the table output
kable2 <- function(x) {
  datatable(x)
     }


# Plot color palette 
colors <- c(
 "#FFBE98",
 "#F05A7E",
 "#125B9A",
 "#0B8494"
)


# Set global options for ggplot colors
thematic::thematic_rmd(
  font = "auto", 
  qualitative = colors
  )

# Set global ggplot2 theme
theme_set(theme_minimal())

# Histogram
plot_hist <- list(
  geom_histogram(aes(y= after_stat(density)), fill=colors[1], color=colors[4]),
  geom_density(color=colors[2], linewidth=1),
  theme_light(),
  theme(plot.title = element_text(hjust = 0.5))
)


# Line plot
plot_line2 <- list(
  geom_line(color = colors[1]),
  scale_color_manual(values = c(colors[1], colors[2])),
  guides(color = "none"),
  theme_light(),
  theme(
    plot.title = element_text(hjust = 0.5)
    )
)


```



# Methodology


## Datasets

Player stats
https://www.basketball-reference.com/


Salaries
The WNBA players' stats were scraped form:
https://www.spotrac.com/wnba/rankings/

CPI:
https://stats.oecd.org




## Variables


Variables imported as part of the players' stats

| Abbreviation | Description                            |
|--------------|----------------------------------------|
| Pos          | Position                               |
| G            | Games                                  |
| MP           | Minutes Played                         |
| GS           | Games Started                          |
| FG           | Field Goals                            |
| FGA          | Field Goal Attempts                    |
| FG%          | Field Goal Percentage                  |
| 3P           | 3-Point Field Goals                    |
| 3PA          | 3-Point Field Goal Attempts            |
| 3P%          | 3-Point Field Goal Percentage          |
| 2P           | 2-Point Field Goals                    |
| 2PA          | 2-point Field Goal Attempts            |
| 2P%          | 2-Point Field Goal Percentage          |
| FT           | Free Throws                            |
| FTA          | Free Throw Attempts                    |
| FT%          | Free Throw Percentage                  |
| ORB          | Offensive Rebounds                     |
| TRB          | Total Rebounds                         |
| AST          | Assists                                |
| STL          | Steals                                 |
| BLK          | Blocks                                 |
| TOV          | Turnovers                              |
| PF           | Personal Fouls                         |
| PTS          | Points                                 |



# Data Import and Cleaning


## Salaries

```{r}
# Import salary dataset
salaries <- read_csv("WNBA - salaries.csv")

salaries %>% 
  glimpse()
```



```{r}

salaries %>% 
  is.na() %>% 
  colSums()

```



```{r}
duplicates <- salaries %>% 
  duplicated() %>% 
  sum()




```

There were `r duplicates` duplicates in the salary dataset.

```{r}
salaries[duplicated(salaries),]
```

The only duplicate concers Sylvia Fowles.
According to spotrac (url: https://www.spotrac.com/wnba/player/_/id/29907/sylvia-fowles#:~:text=Sylvia%20Fowles%20signed%20a%203%20year%20%2C%20%24333%2C540%20contract%20with%20the,average%20annual%20salary%20of%20%24111%2C180.) she extended her contract averaging 115k dollars a year. 
As such this duplicate seems to be a random error and is safe to be removed.


```{r}
# Remove duplicates
salaries <- salaries %>%
  rename(year = Year) %>%
  unique()


# Format salaries to double
salaries$Salary

salaries <- salaries %>% 
  mutate(
    Salary = str_replace_all(Salary, pattern = "\\$|,", "") %>% as.double()
  )


  

```







### Adjusting salaries for inflation

Our dependent variable is salary. As we have data from several years, we would like to adjust the values for inflation. This way they will reflect a more "*true*" value across years.


```{r}
salaries %>%
  count(year) %>% 
  kable2()
```


We have salary data for years `r min(salaries$year)` - `r max(salaries$year)`.


```{r}
cpi <- read_csv("CPI.csv")
cpi2 <- cpi %>%
  filter(
    Measure == "Index",
    Country == "United States",
    Subject == "CPI: 01-12 - All items",
    Time %in% c(as.character(2018:2022), str_c("Q", 1:3, "-2023"))
    ) %>%
  select(Time, Value) %>%
  mutate(Time = str_replace(Time, "Q[1-3]-", "")) %>%
  group_by(Time) %>%
  summarise(
    CPI = mean(Value) # We take the mean of the quaterly 2023 data
  ) %>%
  mutate(year = as.double(Time)) %>%
  select(-Time)
```

Here is the formula for the inflation adjustment factor:

$$ Inflation\; Adjustment\; Factor = \frac{CPI \; in\; Base\; Year}{CPI \; in\;
Current\; Year} $$

We choose 2023 as the base year. 
Therefore, we will adjust the salaries from previous years (2018 to 2022) to reflect their value in 2023 dollars.


```{r}
CPI_base <- cpi2 %>% 
  filter(year == 2023) %>% 
  pull(CPI)
  

salaries <- salaries %>%
  left_join(cpi2) %>%
  mutate(
    iaf = CPI_base / CPI,
    salary_adj = Salary * iaf
  ) %>% 
  select(-c(CPI, iaf))

# Extract a single player that played in most years
player_example <- salaries %>% 
  count(Player) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  pull(Player)

salaries %>% 
  select(Player, year, Salary, salary_adj) %>% 
  filter(Player == player_example) %>% 
  kable2()

```

As expected, the salaries for all years expect for the current (base) year increased, as to reflect their value in terms of the value of 2023 dollar.



## WNBA stats


```{r}

# Import codebook
codebook <- read_excel("codebook.xlsx") %>% 
  select(-1)

codebook <- codebook %>% 
  arrange(variable) 

view(codebook)

# List player stats datasets for all years
stats_files <- list.files("players stats")

# Vector for import
stats_import <- str_c("players stats/", stats_files)

# Extract year
wnba_year <- str_extract(stats_files, pattern = "[0-9]+") %>% 
  as.double()

# Import all files
stats_imported <- stats_import %>% 
  map(read_excel)

# Check compatibility
stats_imported %>% 
  map(names) %>% 
  reduce(setdiff)
```

There are no differences in names of the datasets when compared one after another


```{r}

stats_imported %>% 
  map_dbl(length) %>% 
  unique()
```

All datasets have the same number of variables


```{r}
# Combine all WNBA stats datasets into one
wnba_stats <- stats_imported %>% 
  map2(
    wnba_year,
    ~mutate(.x, year = .y)
  ) %>% 
  reduce(add_row)

```


```{r}

glimpse(wnba_stats)

```


```{r}
wnba_stats %>% 
  duplicated() %>% 
  sum()

```


```{r}

wnba_stats %>% 
  select(where(is.character)) %>% 
  select(-Player) %>% 
  map(unique)


```


### Correcting vector type

```{r}

wnba_stats <- wnba_stats %>% 
  mutate(
    across(ends_with("%"), ~as.double(.))
  )

```


### Summary stats

```{r}

wnba_stats %>% 
  get_summary_stats()

```


### Correcting duplicates 

#### Duplicated variables

```{r}

# Correcting duplicate variables
duplicated_vars <- wnba_stats %>% 
  as.list.data.frame() %>% 
  duplicated() 


wnba_stats[, duplicated_vars]




  

```

Two variables are duplicated. 


```{r}


# Remove duplicates
wnba_stats <- wnba_stats[, !duplicated_vars]


```



#### Duplicated observations

```{r}

wnba_stats %>% 
  duplicated() %>% 
  sum()

```



```{r}

wnba_stats %>% 
  select(Player, year) %>% 
  duplicated() %>% 
  sum()

```



```{r}
player_duplicates <- wnba_stats %>% 
  count(Player, year) %>% 
  filter(n >= 2) %>% 
  arrange(desc(n))

player_duplicates %>% 
  kable2()

```

We have `r nrow(player_duplicates)` cases in which a given player in a given year had more than one set of stats.


```{r}

wnba_stats %>% 
  count(Player, Team, year) %>% 
  filter(n >= 2)




```


This can be explained by some of the players changing their team at least once in a given year.

This raises a methodological question. 
Given that the salaries dataset shows salaries for a given year, in order to combine those with wnba stats each player should have one summarized set of stats for a given year.

A possible solution is to sum stats over a given year for those players who played in more than one team during this year. 

The only limitation would be the position variable, as it is a categorical variable. 

```{r}

wnba_stats %>% 
  count(Player, Pos, year) %>% 
  filter(n >= 2) %>% 
  count(Player, year) %>% 
  filter(n >= 2)

```

However, each player that changed team in a given year nevertheless remained playing on the same position.



```{r}

wnba_summarized <- wnba_stats %>% 
  group_by(Player, year) %>% 
  select(-ends_with("%")) %>% # Exclude percentage variables
  summarise(
    across(where(is.double), ~sum(.))
  ) %>% 
  ungroup()

wnba_position <- wnba_stats %>% 
  group_by(Player, year) %>%
  select(Pos) %>% 
  slice(1) %>% 
  ungroup()
  

wnba_stats2 <- wnba_position %>% 
  left_join(wnba_summarized)

```

```{r}
# create vector for filtering over pairs of successes and attemps
pct_filter <- wnba_stats %>% 
  select(ends_with("%") )%>% 
  names() %>% 
  str_replace("%", "") 

# Vector to loop over
wnba_pct <- list()

# Obtain percentages after data modification
for (var in pct_filter) {
  
  # Select a pair of attemps and successes
  vars <- wnba_stats2 %>% 
    select(starts_with(var)) %>% 
    select(sort(names(.)))
  
  # Calculate percentage of successes over attemps
  pct <- vars[, 1] / vars[, 2]
  
  wnba_pct[[var]] <- pct
  
  
}

wnba_pct <- wnba_pct %>% 
  reduce(add_column) %>% 
  rename_with(~str_c(., "%"))


wnba_stats3 <- wnba_stats2 %>% 
  add_column(wnba_pct) 

wnba_stats3 %>% 
  select(starts_with(pct_filter[1]))

```



### Missing data


```{r}

wnba_missing <- wnba_stats3 %>% 
  is.na() %>% 
  colSums() %>% 
  sort(decreasing = T) %>% 
  keep(~. != 0)

wnba_missing %>% 
  enframe(name = "variable", value = "missing_sum")
  

```

Missing data were observed only for the variables measuring percentage of successes in a given performance category.

Let's inspect observations in terms of the absolute success and overall attempts separately
for each category only for the missing data.


```{r}
## Inspect all missing observations ##



# Empty list
wnba_NAs <- list()

# Obtain separate datasets with missing variables for each set of variables
for (i in seq_along(pct_filter)) {
  
  wnba_NAs[[i]] <- wnba_stats3 %>% 
    select(starts_with(pct_filter[i])) %>% 
    filter(if_any(everything(), ~is.na(.))) %>% 
    select(sort(names(.))) %>% 
    set_names(c("Absolute successes", "Pct successes", "Total attemps")) %>% 
    mutate(category = pct_filter[[i]])
  
}


wnba_NAs %>% 
  map(unique) %>% 
  reduce(add_row) %>% 
  select(category, everything())
```

As we can see, missing data for percentage of success occurred only in situations when there were zero total attempts. 
This makes sense, because you can obtain a proportion out of nothing.


```{r}
pct_filter %>% 
  map(select, .data = wnba_stats3)

wnba_chunks <- pct_filter %>% 
  map(~select(wnba_stats3, starts_with(.))) %>% 
  map(drop_na)


wnba_chunks %>% 
  map(cor_test)
```


```{r}
wnba_chunks %>% 
  map(plot)


```

There are strong linear associations between successes and attempts for each category of player's performance, above 0.9 high.
In case of linear models, it would be necessary to remove either of the two for each pair, in order to avoid collinearity. 

Interestingly, the relationship between the percentage of successful attempts and the absolute values of the sucessess and attempts is not linear. 
The majority of observations had a visible middle with 2P and FG being located mostly between 20% and 70% of successful attempts and
3P between 15% and 50%, showing in the respective areas also a relatively large dispersion of absolute attempts and successes.
Players below or above the respective ranges were noticeably similar to one another in terms of absolute attempts and successes having values close to zero.
This implies that having just a few attempts makes the player's percentage of successes a less objective performance measure, as one or two shots can decide whether a player has 0 or 100% success rate.

The exception is for FT (free throws) for which majority of the observations had a success rate above 60% which was also associated with a higher variance on both absolute measures - attempts and successes. 
Given that a free throw is unhindered and always from the same position, it would on average easier to succeed in a given attempt. 


```{r}

wnba_chunks %>% 
  map(
    ~filter(., .[[names(.)[[3]]]] <= 0.2 | .[[names(.)[[3]]]] >= 0.70)
    ) %>% 
  map(
    get_summary_stats
  )

```


```{r}
wnba_chunks %>% 
  map(
    ~filter(., .[[names(.)[[2]]]] > 50)
    ) %>% 
  map(plot)
```





```{r}

wnba_chunks %>% 
  map(select, 1:2) %>% 
  map(cor_test)

```

This poses a methodological trade-off dilemma.
On one hand, the percentage index by showing the proportion of success to all attempts, seems to capture better the effectiveness of a player. 
On the other, it seems to at the same time favor and neglect those players who had just several attempts in total.

It also raises the question what to do with `NA` values. 



```{r}

wnba_stats3

```





## Combine datasets



```{r}
# Move salaries one year back
salaries_back <- salaries %>% 
  mutate(year = year - 1)
```

Assuming a logical perspective, we would expect that salary would be a consequence of player's prior overall performance.
Therefore, before combining datasets, we should move salaries one year back to associate it with stats from the year prior to the year of salary.


```{r}
# Combine datasets
wnba_combined <- salaries_back %>% 
  left_join(wnba_stats3) 


wnba_combined %>% 
  is.na() %>% 
  colSums() %>% 
  sort(decreasing = T) %>% 
  enframe(name = "variable", value = "Number of missing values") %>% 
  kable2()

```



```{r}

# Select only complete observations
wnba_combined2 <- wnba_combined %>% 
  filter(!if_all(-names(salaries), is.na)) %>% 
  rename(
    Games = G...4,
    Minutes = MP...5
  ) %>% 
  mutate(
    year = factor(year),
    Pos = factor(Pos)
    )

# Remove the unadjusted salary and player name
wnba_complete <- wnba_combined2 %>% 
  select(-c(Salary, Player)) %>% 
  select( salary_adj, everything())

# Adjust variable names to make them comptabile with all modelling functions



```



```{r}

wnba_complete %>% 
  is.na() %>% 
  colSums() %>% 
  keep(~. != 0) %>% 
  enframe(name = "variable", value = "Number of missing values") %>% 
  kable2()

```


We are still left with a substantial number of observations that have zero attempts in at least one of the four performance categories.

# EDA

## Summary stats

```{r}

wnba_complete %>% 
  get_summary_stats()

```


## Distribution of the variables

```{r}

pivoted_everything <- wnba_complete %>% 
  pivot_longer(-c(Pos, year)) 

pivoted_everything %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~name, scales = "free")


```






```{r}

pivoted_everything %>% 
  ggplot(aes(value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")


```




```{r}

wnba_complete %>% 
  freq_table(Pos) %>% 
  ggplot(aes(Pos, prop)) +
  geom_bar(stat = "identity")


```


## Salary over other variables


### Salary before and after adjustment

```{r}

# Obtain means, medians, and percentage change of both, for salaries
salaries_stats <- wnba_combined2 %>% 
  select(year, contains("salary")) %>% 
  group_by(year) %>% 
  get_summary_stats() %>% 
  select(year, variable, mean, median) %>% 
  group_by(variable) %>% 
  mutate(
    mean_change = (mean - lag(mean)) / lag(mean),
    median_change = (median - lag(median)) / lag(median),
      ) %>% 
  mutate(across(c(mean_change, median_change), ~replace_na(., 0)))
  

# Plot absolute changes in salaries
salaries_stats %>% 
  pivot_longer(c(mean, median)) %>% 
  ggplot(aes(year, value, color = variable, group = variable)) +
  geom_point() +
  geom_line() +
  facet_wrap(~name)

```


The absolute change in salaries over the years is higher for salaries before the adjustment for inflation, which is especially visible since 2019.
Interestingly, the mean absolute change was negative for the adjusted salary between 2020 and 2021.



```{r}

# Plot percentage changes for both statistics
salaries_stats %>% 
  pivot_longer(c(mean_change, median_change)) %>% 
  ggplot(aes(year, value, color = variable, group = variable)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  facet_wrap(~name)


```

The difference in rate of change between adjusted and unadjusted values of salaries is even more clear when presented in the percentage change.
However, even after the adjustment, we can see that the salaries of WNBA players grew, with the expection around 2020, which could be due to difficulties COVID-19 related difficulties in organizing matches.


### Salary throughout years


```{r}

wnba_complete %>% 
  ggplot(aes(year, salary_adj)) +
  geom_boxplot()


```

### Salary based on the position



```{r}

wnba_complete %>% 
  ggplot(aes(Pos, salary_adj)) +
  geom_boxplot()


```



### Salary based on stats


```{r}

pivoted_Onsalary <- wnba_complete %>% 
  select(where(is.double)) %>% 
  pivot_longer(-salary_adj)


pivoted_Onsalary %>% 
  ggplot(aes(value, salary_adj)) +
  geom_point(alpha = 1/3) +
  geom_smooth(se = F) +
  facet_wrap(~name, scales = "free", ncol = 5)

```





```{r}

pivoted_Onsalary %>% 
  ggplot(aes(value, salary_adj)) +
  geom_hex() +
  geom_smooth(se = F) +
  facet_wrap(~name, scales = "free", ncol = 5)

```

None of the continuous predictors has a linear relationship with the salary variable.
Based on that it is to predict that variants of tree models will perform better.


### Correlations between predictors


```{r}

cor_predictors <- wnba_complete %>% 
  select(-c(salary_adj, year, Pos)) %>% 
  cor_test(method = "pearson") %>% 
  filter(cor != 1) %>% 
  arrange(desc(abs(cor))) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% 
  select(-c(statistic, method))

cor_predictors %>% 
  kable2()

```

There is a substantial number of highly correlated predictors.
First and foremost, we see almost perfect correlations between successes and attempts for each performance category pair.
Similarly, there are very strong to near perfect correlations between number of points and some of the performance metrics, specifically, 2-points field goals, field goals, and free throws.




```{r}
cor_predictors %>% 
  filter(var1 %in% pct_filter, var2 %in% pct_filter  )



```



```{r}

cor_predictors %>% 
  filter(abs(cor) >= 0.8) %>% 
  count(var1, name = "High correlations count")  %>% 
  arrange(desc(`High correlations count`)) %>% 
  kable2()

```

Field goals, field goals attempts and points were the predictors with the highest count of strong correlations, posing highest risk for multicollinearity.



# Modelling



## Features modification


```{r}
# Rename variables to make  them compatible with all modelling functions
wnba_complete <-  wnba_complete %>% 
  rename_with(~str_replace_all(., "2", "Two_")) %>% 
  rename_with(~str_replace_all(., "3", "Three_")) %>% 
  rename_with(~str_replace_all(., "%", "_pct")) %>% 
  mutate(Pos = str_replace(Pos, "-", "_") %>% factor())


# Turn factors into a dummy variable format
model.matrix(~ ., data = wnba_complete) %>% 
  as_tibble() %>% 
  select(-1)
```


## General setup 



### General parameters 

```{r}

# Extract values from parameters
log_transform <- params$log_transform # Add datasets with log transformed outcome
arrange_metric <- params$arrange_by # Performance metric by which the models should be arranged

# Arrange models by the metric of choice
arrange_own <- function(data) {
  if (arrange_metric == "Rsquared") {
    arrange(data, desc(Rsquared))
  } else {arrange(data, pick(arrange_metric))}
}


```



### Data separation


```{r}

# Create two versions of datasets
model_full <- wnba_complete %>% 
  mutate(across(ends_with("pct"), ~replace_na(., 0))) 

model_reduced <- wnba_complete %>% 
  select(-ends_with("pct"))

model_datasets <- tibble(
  dataset = c("full", "reduced"),
  whole = list(model_full, model_reduced)
)


```


```{r eval=TRUE}

model_datasets <-  model_datasets %>% 
  rename(original = whole ) %>% 
  mutate(log = map(original, function(x) {mutate(x, salary_adj = log(salary_adj))})) %>% 
  pivot_longer(-dataset, names_to = "transformed", values_to = "whole") %>% 
  mutate(
    dataset = str_c(dataset, "_", transformed),
    dataset = factor(dataset, levels = c(
      "full_original", 
      "reduced_original", 
      "full_log", 
      "reduced_log"
    ))
    ) %>% 
  select(-transformed) 



```


```{r}



# Set seed
set.seed(123)


# Set up k-indexing
indices <- sample(1:2, nrow(wnba_complete), prob = c(0.8, 0.2), replace = T)

# Extract general formula
general_formula <- salary_adj ~ .

model_datasets <- model_datasets %>% 
  mutate(
    train = map(whole, ~.[indices == 1,]),
    test = map(whole, ~.[indices == 2,]),
    train_dummy = map(train, ~as_tibble(model.matrix(~ ., .x)[,-1])),
    test_dummy = map(test, ~as_tibble(model.matrix(~ ., .x)[,-1])),
    x_train = map(train, ~model.matrix(general_formula, .x )[, -1]),
    y_train = map(train, ~.x$salary_adj),
    x_test = map(test, ~model.matrix(general_formula, .x )[, -1]),
    y_test = map(test, ~.x$salary_adj),
  )







```




### Functions


```{r}

# Extract model and its performance metrics from a caret object
extract_caret <- function(model) {
   
  # Extract
  model_stats <- model$results %>% 
    select(-ends_with("SD")) %>% 
    as_tibble() %>% 
    mutate(fit = list(model$finalModel))
  
  # Return
  return(model_stats)
  
}


cross_fit <- function(x_data, y_data, ..., tuning, method = "glmnet") {
  
  # train cross-validation models
  model <- train(
    x_data, y_data, ...,
    method = method,
    tuneGrid = tuning,
    trControl = trainControl(method = "cv", number = 10)
  )
  
  
  # Return model and its performance metrics
  return(extract_caret(model))
  
}

```



## Linear models 

Linear models will be fitted using a combination of forward and backward stepwise selections.


```{r}
# Extract regsubsets function
regsubset_own <- function(data, nvmax, method = "forward") {
  regsubsets(
    data = data, 
    nvmax = nvmax, 
    x = general_formula, 
    method = method
    )
}




# Function for predicting values from regsubset object
subset_lm <- function(regsubset, id, data) {
  
  # Extract coefficients
  xvars <- coef(regsubset, id = id) %>% 
    names() %>% 
    keep(~. != "(Intercept)")
  
  
  # Create formula
  form <- str_c("salary_adj ~ ", str_c(xvars, collapse = " + ")) %>% 
    as.formula()
  
  # Set up the train control for 10-fold cross-validation
  train_control <- trainControl(method = "cv", number = 10)
  
  # Fit the linear model using the train function from the caret package
  model <- train(form, data = data, method = "lm", trControl = train_control)
  
  # Return model and its performance stats
  return(extract_caret(model))
  
  
}

# Set seed
set.seed(123)

regsubsets_models <- model_datasets %>% 
  select(dataset, train, train_dummy, test_dummy) %>% 
  mutate(
    id = map(train, ~1:(ncol(.) - 1)),
    predictors_total = map_dbl(train, ncol),
    fit_forward = map2(train, predictors_total, regsubset_own,method = "forward"),
    fit_backward = map2(train, predictors_total, regsubset_own,method = "backward")
    ) 
  

# Fit all best selected models 
regsubsets_models2 <- regsubsets_models %>% 
  select(-c(train,  test_dummy, predictors_total)) %>% 
  pivot_longer(c(fit_forward, fit_backward), names_to = "method", values_to =  "reg_model") %>% 
  unnest(id)  %>% 
  mutate(
    lm = pmap(list(reg_model, id, train_dummy), subset_lm),
  ) %>% 
  unnest(lm) %>% 
  mutate(
    across(where(is.double), ~round(., 3)),
    predictors = id - 1
    ) %>% 
  arrange_own()

# Select the best models
best_lm <- regsubsets_models2 %>% 
  group_by(dataset) %>% 
  slice(1) %>% 
  select(dataset, fit) %>% 
  ungroup()

# 10 best
regsubsets_best10 <- regsubsets_models2 %>% 
  select(dataset, predictors, method, RMSE, MAE, Rsquared) %>% 
  slice(1:10) 

regsubsets_best10 %>% 
  kable2() 

```

#### Best 10 models

Cross-validation revealed that the best 10 linear models explained between 
`r min(regsubsets_best10$Rsquared)`
`r max(regsubsets_best10$Rsquared)` proportion of the total variance,
having between 
`r min(regsubsets_best10$predictors)` and
`r max(regsubsets_best10$predictors)` predictors.


##### Performance against number of predictors


```{r}
# Plot scatterplots of number of variables against R-squared
regsubsets_models2 %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared)) %>%
  ggplot(aes(predictors, Rsquared, color = dataset, alpha = best)) +
  geom_point() +
  facet_wrap(~dataset)
```
The explanatory power of the linear models consistently increases up to around 10 predictors, after which the relationship plateaus.


```{r}
regsubsets_models2 %>% 
  select(dataset, predictors, RMSE, MAE) %>% 
  pivot_longer(c( RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(best = Performance == min(Performance)) %>% 
  ggplot(aes(predictors, Performance, color = dataset, alpha = best)) +
  geom_point() +
  facet_grid(dataset ~ Metric, scales = "free")

```



### Best models coefficients



```{r}
# Obtain coefficients, residuals and diagnostics
best_lm2 <- best_lm %>% 
  mutate(
    coefs = map(fit, ~lm.beta::lm.beta(.) %>% tidy()),
    augmented = map(fit, augment),
    vif = map(fit, vif),
    vif_names = map(vif, names)
    )

lm_coefs <- best_lm2 %>% 
  select(dataset, coefs) %>% 
  unnest() %>% 
  group_by(dataset) %>% 
  arrange(desc(std_estimate)) %>% 
  ungroup() %>% 
  mutate(across(where(is.double), ~round(., 3))) %>% 
  mutate(
    std_estimate2 = if_else(
      p.value <= 0.05, 
      str_c(std_estimate, "*"), 
      as.character(std_estimate) 
    ),
    significant = p.value <= 0.05
  )
```


```{r}
lm_coefs %>% 
  ggplot(aes(std_estimate, term, color = dataset, alpha = significant)) +
  geom_point(position = position_jitter(width = 0, height = 0.3)) 
```

Regardless of the model, field goals, year, and number of games were the strongest predictors of a WNBA player's salary.

The more field goals a player had, the better her salary was. Interestingly, the reverse was true for the number of games. Salaries were visibly higher from 2019 to 2022 than in 2017 and 2018 with a stabilization being achieved beginning with 2019.

Additionally, the number of minutes on the field was an important predictor in models with the log-transformed outcome.


```{r}
# Show standardized coefficients for all models in a table
lm_coefs %>% 
  select(dataset, term, std_estimate2) %>% 
  pivot_wider(names_from = dataset, values_from = std_estimate2) %>% 
  kable2()
  


```



### Model diagnostics


#### Normality assumption


```{r}
best_lm3 <- best_lm2 %>% 
  unnest(augmented) %>% 
  select(-fit)

# Plot histograms
best_lm3 %>% 
  ggplot(aes(.resid)) +
  geom_histogram() +
  facet_wrap(~dataset, scales = "free")
```


```{r}
# Plot Q-Q plots
best_lm3 %>% 
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~dataset, scales = "free")
  



```

Judging by histograms and Q-Q plots, a slight deviation from normality can be observed, with more observations at the tails. However, this slight deviation should not significantly affect the estimation of the coefficients."

#### Homoscedasticity assumption



```{r}

best_lm3 %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~dataset, scales = "free")
  

```

The models fitted to the untransformed data suffered from heteroscedasticity, with a lower error variance at the smaller predicted values. 
This was somewhat, but not entirely, corrected in the models with the log-transformed outcome.
Moreover, all models displayed non-linearity.


#### Multicollinearity




```{r}
# Plot VIF 
best_lm2 %>% 
  select(dataset, vif, vif_names) %>% 
  unnest() %>% 
  ggplot(aes(vif, vif_names, color = dataset)) +
  geom_point(position = position_jitter(height = 0.3))

```

Each of the four best models reported extremely high correlations for the same two predictors:
-   FG (Field goals)
-   FGA (Field goals attempts)

This is in line with the correlation analyses, which revealed that the two predictors had the highest count of correlation above |0.8| with the remaining predictors.

Additionally, models fitted to the datasets with the logarithmitized outcome, reported a high correlation for Minutes, while models fitted to the datasets with the untransformed outcome had a high correlation for TRB (Total Rebounds).


#### Outliers and influential values

##### Cook's distance

```{r}

# Add conditional variables indicating influential and outlier values 
best_lm3 <- best_lm3 %>% 
  mutate(Cook_high = if_else(.cooksd >= 1, T, F)) 

# Plot cook's distance values
best_lm3 %>% 
  ggplot(aes(dataset, .cooksd)) +
  geom_jitter()
  
  

```

No influential values were observed.


##### Outliers

```{r}

# Plot standardized residuals
best_lm3 %>% 
  ggplot(aes(dataset, .std.resid)) +
  geom_jitter()


```

No observations with a standardized residual greater than |x| > 3 were found.


#### Summary 

Even though the best linear models explained a substantial proportion of the outcome's variance, they suffered from moderate heteroscedasticity and, even more importantly, they displayed non-linearity and some of their predictors had a very high VIF. 
All this means that the estimated coefficients are not reliable making the models less trust-worthy.


## Lasso models


```{r}
# Set seed
set.seed(123)



# Create search grid for lambda parameter in lasso
tune_lasso <- data.frame(
  alpha = 1,
  lambda = 10^seq(3, -2, length = 100) 
)


# Start parallel computing
cl <- makeCluster(num_cores)
registerDoParallel(cl)


# Fit lasso models
lasso_models <- model_datasets %>% 
  select(dataset, x_train, y_train) %>% 
  mutate(
    fit_lasso = map2(x_train, y_train, cross_fit, tuning = tune_lasso),
    ) %>% 
  unnest(fit_lasso) %>% 
  arrange_own() 

# Stop parallel computing
stopCluster(cl)
registerDoSEQ()

# Extract best models
best_lasso <- lasso_models %>% 
  group_by(dataset) %>% 
  slice(1) %>% 
  select(dataset, lambda, fit) %>% 
  ungroup()
```


#### Best 10 models


```{r}

# Extract the best 10 models
best_lasso10 <- lasso_models %>% 
  slice(1:10)

# Arrange lasso models in terms of the chosen performance metric
best_lasso10 %>% 
  select(-c(fit, x_train, y_train)) 



```



Cross-validation revealed that the best 10 linear models explained between 
`r min(best_lasso10$Rsquared)`
`r max(best_lasso10$Rsquared)` proportion of the total variance,
with lambda values being between
`r min(best_lasso10$lambda)` and
`r max(best_lasso10$lambda)` predictors.





##### Performance against lambda


```{r}
# Plot scatterplots of number of variables against R-squared
lasso_models  %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared, na.rm = T)) %>%
  ggplot(aes(lambda, Rsquared, color = dataset, alpha = best, shape = best)) +
  geom_point() +
  facet_wrap(~dataset, scales = "free")
```



```{r}
lasso_models %>% 
  select(dataset, lambda, RMSE, MAE) %>% 
  pivot_longer(c( RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(best = Performance == min(Performance)) %>% 
  ggplot(aes(lambda, Performance, color = dataset, alpha = best, shape = best)) +
  geom_point() +
  facet_grid(dataset ~ Metric, scales = "free")

```



#### Best models' coefficients


```{r}

# extract coefficients of only the best lasso models
best_lasso <- best_lasso %>% 
  mutate(
    coefs = map2(fit, lambda, ~coef(.x, s = .y)),
    coefs = map(coefs, ~as.data.frame.matrix(.) %>% rownames_to_column() )
    ) 

# Fit linear models using only those predictors which coefficients weren't reduced to zero
best_lasso_reduced <- best_lasso %>% 
  unnest(coefs) %>% 
  filter(s1 != 0, rowname != "(Intercept)") %>% 
  select(dataset, rowname) %>% 
  nest(.by = dataset) %>% 
  mutate(
    x_vars = map_chr(data, ~str_c(unlist(.), collapse = " + ")),
    formula = str_c("salary_adj ~ ", x_vars),
    formula = map(formula, as.formula)
    ) %>% 
  select(dataset, formula) %>% 
  left_join(model_datasets)  %>% 
  ungroup() %>% 
  mutate(
    fit = map2(formula, train_dummy, ~lm(.x, .y))
  ) %>% 
  select(dataset, test_dummy, fit) 


# Obtain model diagnostic statistics and performance metrics
best_lasso_reduced2 <- best_lasso_reduced %>% 
  mutate(
    RMSE = map2_dbl(fit, test_dummy,  rmse),
    Rsquared = map2_dbl(fit, test_dummy,  rsquare),
    MAE = map2_dbl(fit, test_dummy,  mae),
    map_df(fit, glance),
    vif = map(fit, vif),
    vif_names = map(vif, names),
    augmented = map(fit, augment)
    ) %>% 
  arrange_own()
  
  


```

### Model diagnostics


```{r}

# Extract augmented statistics
best_lasso_augmented <- best_lasso_reduced2 %>% 
  select(dataset, augmented) %>% 
  unnest(augmented)

```


#### Normality assumption

##### Histograms

```{r}
# Plot the histograms of the models' residuals
best_lasso_augmented %>% 
  ggplot(aes(.std.resid)) +
  geom_histogram() +
  facet_wrap(~dataset)


```

The histograms indicate that the distributions of the residuals of all models roughly conformed to the normal distribution.


##### Q-Q plots


```{r}
# Obtain Q-Q plots of the models' residuals
best_lasso_augmented %>% 
  ggplot(aes(sample = .std.resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~dataset)


```


Based on the Q-Q plots, it can be observed that the models fitted on the untransformed salary variable did not deviate significantly from the normal distribution.

The case was somewhat different for the models fitted on the logarithmically transformed dependent variable, where moderate deviations were observed at the edges.


#### Homoscedasticity assumption


```{r}

# Plot scatterplots of the predicted values against the residuals
best_lasso_augmented %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~dataset, scales = "free")

```

All models suffered from heteroscedasticity, although this was less prounced in the case of those which were fitted on the logarithmically transformed outcome.

#### Multicollinearity

```{r}
best_lasso_vif <- best_lasso_reduced2 %>% 
  select(dataset, vif, vif_names) %>% 
  unnest() 



# Plot VIF of the best lasso models
best_lasso_vif %>% 
  unnest() %>% 
  ggplot(aes(vif, vif_names, color = dataset)) +
  geom_point(position = position_jitter(height = 0.3))

```

Assuming the criterion of VIF > 4, All models were reported to have some degree of multicollinearity, with the field goals predictor achieving the highest multicollienarity in each of the models.



#### Outliers and influential values

##### Cook's distance

```{r}

# Add conditional variables indicating influential and outlier values 
best_lasso_augmented <- best_lasso_augmented %>% 
  mutate(Cook_high = if_else(.cooksd >= 1, T, F)) 

# Plot cook's distance values
best_lasso_augmented %>% 
  ggplot(aes(dataset, .cooksd)) +
  geom_jitter()
  
  

```

No influential values were observed.


##### Outliers

```{r}

# Plot standardized residuals
best_lasso_augmented %>% 
  ggplot(aes(dataset, .std.resid)) +
  geom_jitter()


```

No observations with a standardized residual greater than |x| > 3 were found.


#### Summary 

## Tree models


```{r}
# Set seed
set.seed(123)


# Set up tuning grid for tree models
tune_tree <- data.frame(cp = seq(0.001, 0.5, 0.005) )



# Obtain tree models
tree_models <- model_datasets %>% 
  select(dataset, x_train, y_train) %>% 
  mutate(
    fit_tree = map2(
      x_train, 
      y_train, 
      cross_fit, 
      tuning = tune_tree,
      method = "rpart"
      ))


# Arrange models in terms of RMSE
tree_models2 <- tree_models %>% 
  select(-contains("train")) %>% 
  unnest(fit_tree) %>% 
  arrange(RMSE)

# Extract best tree models
best_tree <- tree_models2 %>% 
  group_by(dataset) %>% 
  slice(1) %>% 
  select(dataset, fit) %>% 
  ungroup()
```


```{r}
# Arrange tree models from best to worst
tree_models2 %>% 
  select(-fit) %>% 
  arrange_own() %>% 
  kable2()

```




```{r}



# Plot R-squared of the models
tree_models2 %>% 
  select(-fit) %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared, na.rm = T)) %>% 
  ungroup() %>% 
  ggplot(aes(cp, Rsquared, color = dataset, alpha = best)) +
  geom_point() +
  facet_wrap(~dataset, scales = "free")
```


```{r}

tree_models2 %>% 
  select(-fit) %>% 
  pivot_longer(
    cols = c(RMSE, MAE),
    values_to = "Performance",
    names_to = "Metric"
    ) %>% 
  group_by(dataset, Metric) %>% 
  mutate(best = Performance == min(Performance, na.rm = T)) %>% 
  ungroup() %>% 
  ggplot(aes(cp, Performance, color = dataset, alpha = best, shape = best)) +
  geom_point() +
  facet_grid(dataset ~ Metric, scales = "free")

```

Generally speaking, the tree models with a complexity parameter smaller than 0.1 performed better on the cross-validation prediction.

### Predictors


```{r}

# Function to extract predictors and their importance from the final tree
rpart_predictors <- function(fit) {
  
  # Extract names of the fitted predictors
  predictors <- fit$frame$var %>% 
    keep(~. != "<leaf>")
  
  fit$variable.importance %>% 
    keep_at(~. %in% predictors)
  
  
}

# Plot predictors importance
best_tree %>% 
  mutate(
    importance = map(fit, rpart_predictors),
    importance_names = map(importance, names)
  ) %>%  
  unnest(cols = -fit) %>% 
  ggplot(aes(y = importance_names, x = importance, color = dataset)) +
  geom_point(position = position_jitter(width = 0.2, height = 0.2))


```


### Tree leafs


```{r}


rpart.plot(best_tree$fit[[1]])



```


```{r}

rpart.plot(best_tree$fit[[2]] )

```

### Summary

Overall, the tree models performed noticeably worse in comparison to the linear models. 


## Random forest


```{r}
# Tuning grid for random forest
tuning_forest <- data.frame(mtry = seq(2, ncol(model_full), 2))
```


```{r eval=params$evaluate_chunks}
# Set seed
set.seed(123)


# Start parallel computing
cl <- makeCluster(num_cores)
registerDoParallel(cl)




model_datasets$x_train[[1]]

# Fit random forest models
forest_models <- model_datasets %>% 
  expand_grid(ntree = c(100, 150, 200, 250, 300, 400, 500, 800, 1000)) %>% 
  select(dataset, x_train, y_train, ntree) %>% 
  mutate(fit_forest = pmap(
    list(x_data = x_train, y_data = y_train, ... = ntree), 
    cross_fit, 
    tuning = tuning_forest, 
    method = "rf")
    )

# forest_models2 <- forest_models 

# Stop parallel computing
stopCluster(cl)
registerDoSEQ()
```


```{r eval=params$evaluate_chunks}
# Export random forest object to avoid long processing time
save(forest_models, file = "forest_models.RData")

```



```{r}

load("forest_models.RData")




```

### Random forest models ranking

```{r}
# Extract best forest models for each dataset type
best_forest <- forest_models %>% 
  unnest(fit_forest) %>% 
  group_by(dataset) %>% 
  arrange_own() %>% 
  select(dataset, fit) %>% 
  slice(1) %>% 
  ungroup()

```


```{r}

# Extract model performance metrics by ntree and mtry
forest_fitted <- forest_models %>% 
  unnest(fit_forest) %>% 
  select(dataset, ntree, mtry, RMSE, Rsquared, MAE) %>% 
  arrange_own() 

# Best 10 forest models
best_forest10 <- forest_fitted %>% 
  slice(1:10)

# Show all models
forest_fitted %>% 
  kable2()

```
Cross-validation revealed that the best 10 random forest models explained between 
`r min(best_forest10$Rsquared)`
`r max(best_forest10$Rsquared)` proportion of the total variance,
with mtry being between
`r min(best_forest10$mtry)` and
`r max(best_forest10$mtry)` predictors.

Except for one model, the preferable number of trees for growing were between 100 and 150.



#### Compare random forest models in terms of r2

```{r}

# Add information about best fit for each model type based on R2
forest_rsquared <- forest_fitted %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared)) %>% 
  ungroup() 

# Compare tuning parameters in terms of the R-squared on a heatmap
forest_rsquared %>% 
  ggplot(aes(mtry, factor(ntree))) +
  geom_tile(aes(fill = Rsquared)) +
  geom_point(data = filter(forest_rsquared, best), color = "red") +
  facet_wrap(~dataset, scales = "free") +
  labs(y = "Number of trees to grow",x = "Number of variables randomly sampled") +
  theme(legend.position = "top")

```


Overall, models with higher mtry values were associated with a higher R-squared. 
A more pronounced difference was observed for the number of trees parameter with higher number of trees leading to better performance in the full untransformed dataset and smaller number of trees being more effect for the reduced untransformed dataset.

#### Compare random forest models in terms of RMSE and MAE

```{r}

# Add information about best fit for each model type based on R2
forest_performance <- forest_fitted %>% 
  pivot_longer(cols = c(RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(
    best = Performance == min(Performance),
    transformed = if_else(str_detect(dataset, "log"), "Untransformed", "Transformed")
    ) %>% 
  ungroup() %>% 
  nest(.by = c(Metric, transformed)) 
  




# Create list for all plots
plots_forest <- list()


# Creat heatmaps for each metric and separately for transformed and untrasformed datasets
for (i in 1:nrow(forest_performance)) {
  
  plots_forest[[i]] <-  forest_performance$data[[i]] %>% 
    # filter(str_detect(dataset, "original"), Metric == "RMSE") %>% 
    ggplot(aes(mtry, factor(ntree))) +
    geom_tile(aes(fill = Performance)) +
    geom_point(data = filter(forest_performance$data[[i]], best), color = "red") +
    facet_wrap(~dataset, scales = "free") +
    labs(
      y = "Number of trees to grow",
      x = "Number of variables randomly sampled",
      title = str_c(
        "Comparison of ", forest_performance$transformed[[i]], " models in terms of ", forest_performance$Metric[[i]]
        )
      )
}

```



```{r}
# Compare transformed models in terms of RMSE and MAE
cowplot::plot_grid(plots_forest[[1]], plots_forest[[2]], nrow = 2)
```

When compared in terms of RMSE and MAE, random forest models fitted to the untransformed datasets revealed similarly preferable values of `ntree` and `mtry`.

```{r}
# Compare transformed models in terms of RMSE and MAE
cowplot::plot_grid(plots_forest[[3]], plots_forest[[4]], nrow = 2)

```


### Predictors importance


```{r}

# Extract predictors' importance for each model
best_forest2 <- best_forest %>% 
  mutate(
    importance_purity = map(fit, ~importance(.) %>% as.data.frame() %>% rownames_to_column())
    )


```


```{r}
# Plot predictors importance values for the untransformed datasets
best_forest2 %>% 
  select(dataset, importance_purity) %>% 
  unnest() %>% 
  filter(str_detect(dataset, "original")) %>% 
  ggplot(aes(y = rowname, x = IncNodePurity, color = dataset)) +
  geom_point()
```
Interestingly, the random forest models revealed games and games started as the most important predictors.
Less important, but still noticeable predictors, were field goals, personal fouls, points and 2-Point Field Goals.

```{r}
# Plot predictors importance values for the transformed datasets
best_forest2 %>% 
  select(dataset, importance_purity) %>% 
  unnest() %>% 
  filter(str_detect(dataset, "log")) %>% 
  ggplot(aes(y = rowname, x = IncNodePurity, color = dataset)) +
  geom_point() 

```




```{r include=FALSE}
# Create a function for extracting y and x values from a partial plot object
extract_partial <- function(partial_data) {

  
  
  # Return the partial data as a data frame
  tibble(
    predictor = partial_data$x, 
    salary = partial_data$y
    )
}



partialPlot(best_forest$fit[[1]], model_datasets$x_train[[1]], "Games") %>% 
  extract_partial()
```


```{r include=FALSE}

# Enable parallel computing
plan(multisession)


# Extract partial predictions for most important predictors
best_forest2 <- best_forest %>% 
  left_join(model_datasets) %>% 
  mutate(
    plots_Games = future_map2(fit, x_train, partialPlot, x.var = "Games", plot = FALSE),
    plots_GS = future_map2(fit, x_train, partialPlot, x.var = "GS", plot = FALSE),
    plots_FG = future_map2(fit, x_train, partialPlot, x.var = "FG", plot = FALSE),
    plots_PTS = future_map2(fit, x_train, partialPlot, x.var = "PTS", plot = FALSE)
  ) %>% 
  select(dataset, starts_with("plots")) %>% 
  pivot_longer(-dataset, names_to = "predictor_name", values_to = "partial_predictions") %>% 
  mutate(
    predictor_name = str_replace(predictor_name, "plots_", ""),
    partial_predictions = future_map(partial_predictions, extract_partial)
    ) 

plan(sequential)

```



```{r }


# Plot partial predictions of the selected variables
best_forest2 %>% 
  unnest() %>% 
  mutate(salary = if_else(str_detect(dataset, "log"), exp(salary), salary)) %>% 
  ggplot(aes(predictor, salary, color = dataset, group = dataset)) +
  geom_line() +
  facet_wrap(~predictor_name, scales = "free")


```


## Boosted tree

```{r}

# Set seed
set.seed(123)


# Tuning grid for boosted trees
tune_boosted <- expand_grid(
      nrounds = c(25, 50, 100, 200),
      max_depth = seq(1, 9, 2),
      eta = c(0.05, 0.1, 0.15, 0.2),
      gamma = 0,
      colsample_bytree = 1,
      min_child_weight = 1,
      subsample = 1
      )  
  
  


# Set up parallel computing
cl <- makeCluster(num_cores)
registerDoParallel(cl)



boosted_models <- model_datasets %>% 
  select(dataset, x_train, y_train) %>% 
  mutate(
    fit_boost = map2(x_train, y_train, cross_fit, tuning = tune_boosted, method = "xgbTree")
  )


# Stop parallem computing
stopCluster(cl)
registerDoSEQ()



best_boosted <- boosted_models %>% 
  unnest(fit_boost) %>% 
  group_by(dataset) %>% 
  arrange_own() %>% 
  select(dataset, fit) %>% 
  slice(1) %>% 
  ungroup()


```



```{r}

boosted_models2 <- boosted_models %>% 
  select(dataset, fit_boost) %>% 
  unnest(fit_boost) %>% 
  select(dataset, eta, max_depth, nrounds, RMSE, MAE, Rsquared) %>% 
  arrange_own()

```


### Plot

```{r}
boosted_rsquared <- boosted_models2 %>% 
  mutate(
    eta_nround = interaction(eta, nrounds)
  ) %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared)) %>% 
  ungroup() 

# Compare tuning parameters in terms of R-squared on a heatmap
boosted_rsquared %>% 
  ggplot(aes(max_depth, eta_nround)) +
  geom_tile(aes(fill = Rsquared)) +
  geom_point(data = filter(boosted_rsquared, best), color = "red") +
  facet_wrap(~dataset, scales = "free") +
  labs(y = "Number of trees to grow",x = "Number of variables randomly sampled")
  
  
```

### Variable importance

```{r}

best_boosted$fit

best_boosted %>% 
  mutate(importance = map(fit, xgb.importance))
  

```




# Final model comparison

## Functions for performance metrics

```{r}

round_number <- 3

# Obtain Mean-squared error
rmse_own <- function(predicted, true_y) {
  
  
  # Obtain mse
  rmse <- sqrt(mean((true_y - predicted)^2))
  rmse <- round(rmse, round_number)
  
  return(rmse)
  
}


# Obtain Mean-median error
mae_own <- function(predicted, true_y) {
  
  # Obtain mse
  mae <- mean(abs(true_y - predicted))
  mae <- round(mae, round_number)
  
  return(mae)
  
}

# Obtain R-squared
rsquare_own <- function(predicted, true_y) {
  
  
  # Obtain mse
  r2 <- 1 - (sum((true_y - predicted) ^ 2) / sum((true_y - mean(true_y)) ^ 2))
  r2 <- round(r2, round_number)
  
  return(r2)
  
}


```



## Fit models onto test dataset

```{r}

# Remove unnecesary columns for best models mering
best_lasso_reduced <- best_lasso_reduced %>% 
  select(-test_dummy)

best_tested <- best_boosted %>% 
  add_row(best_forest) %>% 
  add_row(best_lasso_reduced) %>% 
  add_row(best_tree) %>% 
  add_row(best_lm) %>% 
  left_join(model_datasets)  %>% 
  mutate(
    fit_class = map_chr(fit, class),
    test = if_else(fit_class %in% c("lm", "rpart"), test_dummy, x_test)
    ) %>% 
  mutate(
    predicted = map2(fit, test, predict),
    RMSE = map2_dbl(predicted, y_test, rmse_own),
    MAE = map2_dbl(predicted, y_test, mae_own),
    Rsquared = map2_dbl(predicted, y_test, rsquare_own)
  ) %>% 
  arrange_own()


best_tested %>% 
  select(dataset, fit_class, RMSE, MAE, Rsquared) %>% 
  kable2()

```



### Importance


```{r}

  xgboost::xgb.importance(best_tested$fit[[1]])


```




https://stackoverflow.com/questions/30097730/error-when-using-predict-on-a-randomforest-object-trained-with-carets-train

Use non formula version for train to avoid the problem with predict random forest