---
title: "Predicting salaries of WNBA players"
author: Caban
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 3
    toc_float: yes
urlcolor: blue
params:
  log_transform: true
  evaluate_chunks: false
  arrange_by:
    label: "Performance metric to arrange the models by"
    value: "Rsquared"
    input: select
    choices: ["RMSE", "MAE", "Rsquared"]
  
---


```{r setup, include=FALSE}

# Set knitr options
knitr::opts_chunk$set(
  echo = T, 
  fig.width=8, 
  # fig.height=4, 
  warning = F, 
  dpi = 300,
  message = F,
  comment = NA, 
  fig.showtext = TRUE
  )





if(!require('pacman')) {install.packages('pacman')}

# Load all libraries
pacman::p_load(showtext, ggcorrplot, pdp, modelr, xgboost, DT, readxl,leaps,furrr, rstatix, broom, bestglm, glmnet, leaps, car,  pROC, caret, tree, rpart, randomForest, rpart.plot, doParallel, tidyverse)


# Set up parallel backend
num_cores <- detectCores() - 1


```






```{r}

# create functions and general ggplot vectors to streamline code and make it more reproducible


### General figure parameters
height_faceted <- 10


# Function to control the table output
kable2 <- function(x) {
  datatable(x)
     }


# Plot color palette 
colors <- c(
 "#FFBE98",
 "#F05A7E",
 "#125B9A",
 "#0B8494"
)

# Set the dot color for performance metrics comparison
dot_color <- "red"



# Set global options for ggplot colors
thematic::thematic_rmd(
  font = "auto", 
  qualitative = colors
  )

# Set global ggplot2 theme
theme_set(theme_minimal())





```



# Introduction

Sport in general is a domain in which competence is measured by well defined and precisely measured criteria. At the simplest level, winning is a direct reflection of someone's performance.
Team sports follow the same basic principles, although in their context, it is more difficult to assess individual competence. After all, it is the team who wins, and not individual players. However, most team sports consist of a set of different plays and behaviors that can be treated as specific performance measures.
One such sport is basketball in which player's performance is measured by the number of goals, steals, or blocks, to name a few.
The most popular leagues, like NBA, have a long history and been well studied in different aspects.
In contrast, WNBA, women's equivalent, was established relatively recently, in 1996, and as such has been less analyzed.
The goal of the following analysis was to assess which and how women's basketball players performance measures predict their salary. 

The WNBA players' performance stats were imported from the basketball [reference stats](https://www.basketball-reference.com/), while their salaries were scraped from [spotrac](https://www.spotrac.com/wnba/rankings/).
The data spans years from 2017 to 2023. 
In order to account for inflation, salary was adjusted using the CPI index. 
This is an observational study and therefore a clear cause-and-effect relationship cannot be established. Nevertheless, from a theoretical stand point, it makes sense to infer, at least partially, that performance stats translate to salary and to better approximate such relationship, salary was predicted by performance stats from one year earlier.

Given that the data suffered from high multicollinearity and that variables representing a percentage of successful goal attempts posed a methodogolical limitations for players who did not have any attempts in a given performance stats, four versions of data were created. In two versions salary, the dependent variable, was log-transformed, and in each of the pairs - log-transformed and original - one of the datasets was reduced by the four percentage variables. 

Each dataset was trained and then tested using 
Stepwise Selection Linear Regression, 
Lasso Regression, 
Tree model,
Random Forest, and
xgb boosted tree
models.
The analysis revealed that the best prediction was achieved by xgb boosted tree model fitted to the reduced dataset with untransformed salary, followed by the same model fitted to the reduced dataset with log-transformed salary.

The best predictors of salary were number of Field Goals, Games Started, and Two-Point Field Goals, which were associated positively, and number of Games and Personal Fouls, which were associated negatively.







# Methodology




## Datasets

Player stats
https://www.basketball-reference.com/


Salaries
https://www.spotrac.com/wnba/rankings/

CPI:
https://stats.oecd.org




## Variables


Variables imported as part of the players' stats

| Abbreviation | Description                            |
|--------------|----------------------------------------|
| Pos          | Position                               |
| G            | Games                                  |
| MP           | Minutes Played                         |
| GS           | Games Started                          |
| FG           | Field Goals                            |
| FGA          | Field Goal Attempts                    |
| FG%          | Field Goal Percentage                  |
| 3P           | 3-Point Field Goals                    |
| 3PA          | 3-Point Field Goal Attempts            |
| 3P%          | 3-Point Field Goal Percentage          |
| 2P           | 2-Point Field Goals                    |
| 2PA          | 2-point Field Goal Attempts            |
| 2P%          | 2-Point Field Goal Percentage          |
| FT           | Free Throws                            |
| FTA          | Free Throw Attempts                    |
| FT%          | Free Throw Percentage                  |
| ORB          | Offensive Rebounds                     |
| TRB          | Total Rebounds                         |
| AST          | Assists                                |
| STL          | Steals                                 |
| BLK          | Blocks                                 |
| TOV          | Turnovers                              |
| PF           | Personal Fouls                         |
| PTS          | Points                                 |




## Positions

| Abbreviation | Position                               |
|--------------|----------------------------------------|
| C            | Center                                 |
| C-F          | Center-Forward                         |
| F            | Forward                                |
| F-C          | Forward-Center                         |
| F-G          | Forward-Guard                          |
| G            | Guard                                  |
| G-F          | Forward-Center                         |





# Data Import and Cleaning


## Salaries

```{r}
# Import salary dataset
salaries <- read_csv("WNBA - salaries.csv")

salaries %>% 
  glimpse() 
```


```{r}


sal_dim <- dim(salaries)

```
There are `r sal_dim[1]` observations in the salary dataset with `r sal_dim[2]` variables.



```{r}

# Check if there are any NAs in the salaries dataset
salaries_na <- salaries %>% 
  is.na() %>% 
  colSums()

```

There are `r sum(salaries_na)` missing data points in total. 


```{r}
duplicates <- salaries %>% 
  duplicated() %>% 
  sum()




```

There are `r duplicates` duplicates in the salary dataset.

```{r}
salaries[duplicated(salaries),]
```

The only duplicate concers Sylvia Fowles.
According to [spotrac](https://www.spotrac.com/wnba/player/_/id/29907/sylvia-fowles#:~:text=Sylvia%20Fowles%20signed%20a%203%20year%20%2C%20%24333%2C540%20contract%20with%20the,average%20annual%20salary%20of%20%24111%2C180) she extended her contract averaging 115k dollars a year. 
As such this duplicate seems to be a random error and is safe to be removed.


```{r}
# Remove duplicates
salaries <- salaries %>%
  rename(year = Year) %>%
  unique()


# Format salaries to double
salaries <- salaries %>% 
  mutate(
    Salary = str_replace_all(Salary, pattern = "\\$|,", "") %>% as.double()
  )


  

```







### Adjusting salaries for inflation

Our dependent variable is salary. As we have data from several years, we would like to adjust the values for inflation. This way they will reflect a more "*true*" value across years.


```{r}
salaries %>%
  count(year) %>% 
  kable2()
```


We have salary data for years `r min(salaries$year)` - `r max(salaries$year)`.


```{r}
# Import cpi data
cpi <- read_csv("CPI.csv")

# Extract and modify part of cpi data to match it with the salary data
cpi2 <- cpi %>%
  filter(
    Measure == "Index",
    Country == "United States",
    Subject == "CPI: 01-12 - All items",
    Time %in% c(as.character(2018:2022), str_c("Q", 1:3, "-2023"))
    ) %>%
  select(Time, Value) %>%
  mutate(Time = str_replace(Time, "Q[1-3]-", "")) %>%
  group_by(Time) %>%
  summarise(
    CPI = mean(Value) # We take the mean of the quaterly 2023 data
  ) %>%
  mutate(year = as.double(Time)) %>%
  select(-Time)
```

Here is the formula for the inflation adjustment factor:

$$ Inflation\; Adjustment\; Factor = \frac{CPI \; in\; Base\; Year}{CPI \; in\;
Current\; Year} $$

We choose 2023 as the base year. 
Therefore, we will adjust the salaries from previous years (2018 to 2022) to reflect their value in 2023 dollars.


```{r}
# Extract the CPI of the reference year to which the value of salaries will be adjusted
CPI_base <- cpi2 %>% 
  filter(year == 2023) %>% 
  pull(CPI)
  

# Add the cpi information to salaries
salaries <- salaries %>%
  left_join(cpi2) %>%
  mutate(
    iaf = CPI_base / CPI,
    salary_adj = Salary * iaf
  ) %>% 
  select(-c(CPI, iaf))
```


```{r}
# Extract a single player that played in most years
player_example <- salaries %>% 
  count(Player) %>% 
  arrange(desc(n)) %>% 
  slice(1) %>% 
  pull(Player)

salaries %>% 
  select(Player, year, Salary, salary_adj) %>% 
  filter(Player == player_example) %>% 
  kable2()

```

As expected, the salaries for all years except for the current (base) year increased, as to reflect their value in 2023 dollars.



## WNBA stats


```{r}

# Import codebook
codebook <- read_excel("codebook.xlsx") %>% 
  select(-1)

codebook <- codebook %>% 
  arrange(variable) 

# view(codebook)

# List player stats datasets for all years
stats_files <- list.files("players stats")

# Vector for import
stats_import <- str_c("players stats/", stats_files)

# Extract year
wnba_year <- str_extract(stats_files, pattern = "[0-9]+") %>% 
  as.double()

# Import all files
stats_imported <- stats_import %>% 
  map(read_excel)

# Check compatibility of stats datasets variable names across years
stats_imported %>% 
  map(names) %>% 
  reduce(setdiff)
```

There are no differences in names of the datasets when compared to one another.


```{r}

# Check compatibility of stats datasets number of variables across years
stats_imported %>% 
  map_dbl(length) %>% 
  unique()
```

All datasets have the same number of variables.


```{r}
# Combine all WNBA stats datasets into one
wnba_stats <- stats_imported %>% 
  map2(
    wnba_year,
    ~mutate(.x, year = .y)
  ) %>% 
  reduce(add_row)

```


```{r}

glimpse(wnba_stats)

wnba_dim <- dim(wnba_stats)

```


The compiled WNBA players' stats dataset consists of `r wnba_dim[1]` observations and `r wnba_dim[2]` variables.


```{r}
# Inspect character vectors
wnba_stats %>% 
  select(where(is.character)) %>% 
  map(unique) %>% 
  map(~.[1:15])
```
There are 4 vectors representing proportion values that are incorrectly read as strings and therefore should be coerced to doubles.


```{r}

# Correcting vector types for 
wnba_stats <- wnba_stats %>% 
  mutate(
    across(ends_with("%"), ~as.double(.))
  )

```



```{r}

# Check unique values for character vectors
wnba_stats %>% 
  select(where(is.character)) %>% 
  select(-Player) %>% 
  map(unique) %>% 
  enframe(name = "Variable", value = "Unique values") %>% 
  mutate(
    `Uniques number` = map(`Unique values`, length),
    `Unique values` = map_chr(`Unique values`, str_c, collapse = ", ")
    ) %>% 
  kable2()


```

There are two character variables, Team and Position, which contain 14 and 7 unique values (categories) respectively.     



```{r}


# Check duplicates
wnba_duplicated <- wnba_stats %>% 
  duplicated() %>% 
  sum()

```

There were `r wnba_duplicated` duplicated rows in the WNBA dataset.



### Summary stats

```{r}

wnba_stats %>% 
  get_summary_stats() %>% 
  kable2()

```


Two pairs of variables have exactly the same values for all descriptive statistics, which suggests they are identical. 


### Correcting duplicates 

#### Duplicated variables

```{r}

# Identify duplicated variables
duplicated_vars <- wnba_stats %>% 
  as.list.data.frame() %>% 
  duplicated() 

# Show duplicated variables
wnba_stats[, duplicated_vars] %>% 
  kable2()




  

```

Two variables are duplicated. 


```{r}


# Remove duplicated variables
wnba_stats <- wnba_stats[, !duplicated_vars]


```



#### Duplicated observations

```{r}

# Check if there are duplicated observations
wnba_stats %>% 
  duplicated() %>% 
  sum()

```


There are no duplicated rows in the WNBA stats dataset.


```{r}

# Check if there are some players who have more than one row of stats for a given year
players_year_dup <- wnba_stats %>% 
  select(Player, year) %>% 
  duplicated() %>% 
  sum()

```


There are `r players_year_dup` cases of additional rows of stats for a given player in a given year.


```{r}

# Identify all cases of in which in a given year a given player has more than one row of stats
player_duplicates <- wnba_stats %>% 
  count(Player, year) %>% 
  filter(n >= 2) %>% 
  arrange(desc(n))

player_duplicates %>% 
  kable2()

```

We have `r nrow(player_duplicates)` cases in which a given player in a given year had more than one row of game stats.


```{r}

# Check if this holds true if we add team to the calculation
wnba_stats %>% 
  count(Player, Team, year) %>% 
  filter(n >= 2)




```


The identified repetition disappear when we include the team variable. 
This suggests that some of the players changed their team a given number of times in a given year.

```{r}
# Create a filter vector for repetitions 
player_filtering <- player_duplicates %>% 
  mutate(repeated_filter = str_c(Player, year)) %>% 
  pull(repeated_filter)

# Show the different teams identified players played in given years
wnba_stats %>% 
  mutate(repeated_filter = str_c(Player, year)) %>% 
  filter(repeated_filter %in% player_filtering) %>% 
  select(Player, Team, year) %>% 
  arrange(Player, year) %>% 
  kable2()

```
For instance, a player named AD Durr played on three different teams during 2022.
This raises a methodological question. 

Given that the salaries dataset shows salaries for a given year for a given player, to combine salaries data with wnba stats, each player should have one summary set of stats for a given year.

A possible solution is to sum the stats over a given year for players who played on more than one team during this year. 

The only limitation would be the position variable, as it is a categorical variable. 

```{r}

# Check if among the players that changed their teams in a given year 
wnba_stats %>% 
  count(Player, Pos, year) %>% 
  filter(n >= 2) %>% 
  count(Player, year) %>% 
  filter(n >= 2)

```

However, each player that changed team in a given year nevertheless remained playing on the same position.
We can therefore summarize their results over given years and add them back to the dataset.
We will have to exclude the percentage variables during the summarization and calculate them once the summarized stats of the identified players are joined with the remaining stats of player stats in a given year.


```{r}


# Obtain summary stats for each player in a given year
wnba_summarized <- wnba_stats %>% 
  group_by(Player, year) %>% 
  select(-ends_with("%")) %>% # Exclude percentage variables
  summarise(
    across(where(is.double), ~sum(.))
  ) %>% 
  ungroup()

# Extract information about position
wnba_position <- wnba_stats %>% 
  group_by(Player, year) %>%
  select(Pos) %>% 
  slice(1) %>% 
  ungroup()
  

# Join position and main stats datasets
wnba_stats2 <- wnba_position %>% 
  left_join(wnba_summarized)

```

```{r}
# create vector for filtering over pairs of successes and attemps
pct_filter <- wnba_stats %>% 
  select(ends_with("%") )%>% 
  names() %>% 
  str_replace("%", "") 

# Vector to loop over
wnba_pct <- vector("list", length(pct_filter)) %>% 
  set_names(pct_filter)

# Obtain percentages after data modification
for (var in pct_filter) {
  
  # Select a pair of attemps and successes
  vars <- wnba_stats2 %>% 
    select(starts_with(var)) %>% 
    select(sort(names(.)))
  
  # Calculate percentage of successes over attemps
  pct <- vars[, 1] / vars[, 2]
  
  wnba_pct[[var]] <- pct
  
  
}

# Join all pct variables into one column
wnba_pct <- wnba_pct %>% 
  reduce(add_column) %>% 
  rename_with(~str_c(., "%"))


# Join main stats and pct datasets
wnba_stats3 <- wnba_stats2 %>% 
  add_column(wnba_pct) %>% 
  rename(
    Games = G...4,
    Minutes = MP...5
  ) # Fix names of specific variables

# Check if the code gave correct results
wnba_stats3 %>% 
  select(starts_with(pct_filter[1])) %>% 
  kable2()

```



### Missing data


```{r}

wnba_missing <- wnba_stats3 %>% 
  is.na() %>% 
  colSums() %>% 
  sort(decreasing = T) %>% 
  keep(~. != 0)

wnba_missing %>% 
  enframe(name = "variable", value = "missing_sum") %>% 
  kable2()
  

```

Missing data were observed only for the variables measuring percentage of successes in a given performance category.

Let's inspect observations in terms of the absolute success and overall attempts separately
for each category only for the missing data.


```{r}
## Inspect all missing observations ##



# Empty list for running a loop
wnba_NAs <- vector("list", length(pct_filter))

# Obtain separate datasets with missing variables for each set of variables
for (i in seq_along(pct_filter)) {
  
  wnba_NAs[[i]] <- wnba_stats3 %>% 
    select(starts_with(pct_filter[i])) %>% 
    filter(if_any(everything(), ~is.na(.))) %>% 
    select(sort(names(.))) %>% 
    set_names(c("Absolute successes", "Pct successes", "Total attemps")) %>% 
    mutate(category = pct_filter[[i]])
  
}


# Show summary of instances where the NA's for pct variables occur
wnba_NAs %>% 
  map(unique) %>% 
  reduce(add_row) %>% 
  select(category, everything()) %>% 
  knitr::kable()
```

As we can see, missing data for percentage of success occurred only in situations when there were zero total attempts. 
This makes sense, because you can't calculate a proportion of something to nothing.


### Performance metrics sets


The missing data for the performance metrics expressed in percentages raises another methodological problem. 
The missing data cannot be imputed given the mathematical nature of those variables.
First, let's inspect each set of performance metrics separately, to see how the three metrics in a given category are related to each other.


```{r}

# Obtain separate data frames for each of the goal performance category sets
wnba_chunks <- pct_filter %>% 
  map(~select(wnba_stats3, starts_with(.))) %>% 
  map(drop_na)





# Obtain ggcorrplots for each sets
ggcorr_sets <- wnba_chunks %>% 
  map(cor) %>% 
  map(~ggcorrplot(., lab = T)) 


do.call(cowplot::plot_grid, ggcorr_sets)
```

There are near perfect linear associations between successes and attempts for each category of player's performance.
In case of the linear models, it would be necessary to remove either of the two for each pair, in order to avoid multicollinearity. 


```{r}

wnba_chunks %>% 
  walk(plot)




```



Interestingly, the relationship between the percentage of successful attempts and the absolute values of the successes and attempts is not linear. 

The majority of the observations were located within a specific and Gaussian-like interval.
Even though those are scatter plots and not histograms, it is interesting to see that the relationships between a given percentage metric and either of the two remaining performance metrics seem to follow a normal distribution. 
This suggests that the percentage metrics also carry information about the "norm" or the "average" skill with 2-Point Goals and Field Goals being located mostly between 30% and 65% and 3-Point Goals between 20% and 45%.



```{r}

# Show min and max values of percentage metrics for players with at least 10 attempts
wnba_stats %>% 
  filter(if_all(c("2PA", "FTA", "3PA", "FGA"), ~. >= 10)) %>% 
  select(ends_with("%")) %>% 
  get_summary_stats() %>% 
  pivot_longer(c(min, max)) %>% 
  ggplot(aes(value, variable, group = variable))  +
  geom_line(color = "black") +
  geom_point() +
  labs(
    title = "Percentage metric values for players who had at least 10 goal attempts",
    y = "",
    x = "Percentage of successes"
    ) +
  scale_x_continuous(label = scales::label_percent())


```




However, the percentage metrics suffer from a critical short-coming.
Players below or above the respective "average" ranges were noticeably similar to one another, with values close to zero for both attempts and successes.
Among players with more at least 10 attempts there were virtually none who had a higher success rate than 70%. A similar case can be made for the lower ranges for 2-Point and Field goals.
This implies that having only a few to several attempts makes a player’s success percentage less objective as a performance measure, as one or two shots can determine whether a player has a 0% or 100% success rate.

The exception is for FT (free throws) for which majority of the observations had a success rate above 60% which was also associated with a higher variance on both absolute measures - attempts and successes. 
Given that a free throw is unhindered and always from the same position, it would be on average easier to succeed in a given attempt. 


```{r eval=FALSE}

# Show summary stats for each sets when the proportion of successes is between 0.2 and 0.7
wnba_chunks %>% 
  map(
    ~filter(., .[[names(.)[[3]]]] <= 0.2 | .[[names(.)[[3]]]] >= 0.70)
    ) %>% 
  map(
    get_summary_stats
  ) %>% 
  reduce(add_row) %>% 
  kable2()

```






This poses a methodological trade-off dilemma.
On one hand, the percentage index, by showing the proportion of successes to all attempts, seems to capture better the effectiveness of a player. 
On the other hand, it seems to favor or overlook players who had just a few attempts in total, with either mostly successes or mostly misses.

It also raises the question what to do with `NA` values. 





## Combine datasets



```{r}
# Move salaries one year back
salaries_back <- salaries %>% 
  mutate(year = year - 1)
```

Assuming a cause and effect relationship, we would expect that salary would be a consequence of player's prior overall performance.
Therefore, before combining datasets, we should move salaries one year back to associate it with stats from the year prior to the year of salary.


```{r}
# Combine datasets
wnba_combined <- salaries_back %>% 
  left_join(wnba_stats3) 


# Select only complete observations
wnba_combined2 <- wnba_combined %>% 
  filter(!if_all(-names(salaries), is.na)) %>% 
  mutate(
    year = factor(year),
    Pos = factor(Pos)
    )

# Check number of missing values of joining the two datasets
wnba_combined %>% 
  is.na() %>% 
  colSums() %>% 
  sort(decreasing = T) %>% 
  enframe(name = "variable", value = "Number of missing values") %>% 
  kable2()

```

Joining the salary and the WNBA stats datasets resulted in `r nrow(wnba_combined) - nrow(wnba_combined2)` observations that lacked data in all performance stats.


```{r}



# Remove the unadjusted salary and player name
wnba_complete <- wnba_combined2 %>% 
  select(-c(Salary, Player)) %>% 
  select( salary_adj, everything())

# Adjust variable names to make them comptabile with all modelling functions



```



```{r}

wnba_complete %>% 
  is.na() %>% 
  colSums() %>% 
  keep(~. != 0) %>% 
  enframe(name = "variable", value = "Number of missing values") %>% 
  kable2()

```


After removing observations with no performance stats, we are still left with a substantial number of observations that have zero attempts in at least one of the four performance categories.

# EDA

## Summary stats

```{r}

wnba_complete %>% 
  get_summary_stats() %>% 
  kable2()

```

The final dataset consisted of `r nrow(wnba_complete)` records and `r ncol(wnba_complete)` variables.



## Distribution of the variables



### WNBA stats before and after reduction



```{r fig.height=height_faceted}

# Pivot the full dataset stats
pivoted_full_stats <- wnba_stats3 %>% 
  select(where(is.double), -year) %>% 
  pivot_longer(everything()) %>% 
  mutate(stats_data = "full")

# Pivot the combined dataset
pivoted_everything <- wnba_complete %>% 
  pivot_longer(-c(Pos, year)) 


# Join old and new pivoted datasets
pivoted_combied <- pivoted_everything %>% 
  select(-c(year, Pos)) %>% 
  mutate(stats_data = "reduced") %>% 
  add_row(pivoted_full_stats)


# Compare old and new distributions 
pivoted_combied %>% 
  ggplot(aes(value, color = stats_data)) +
  geom_density() +
  facet_wrap(~name, scales = "free", ncol = 4) +
  theme(legend.position = "top")

```
There were moderate differences between the distributions of the full and reduced datasets for specific variables.

The most noticeable differences concerned Field Goals and Field Goal Attempts, 2-point goals and 2-point goal attempts, Points, Turnovers, Steals, and Minutes, showing a proportionally smaller number of lower values in the dataset that had to be reduced due to lack of the corresponding salary data.

This suggests that the salary information available on the website from which it was scraped included salaries primarily for better-performing players.

The analyzed sample, therefore, is not representative of the general population and, as such, will reduce the external validity of the following statistical modeling.





### Histograms

```{r fig.height=height_faceted}


pivoted_everything %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~name, scales = "free", ncol = 4)


```


Almost all performance variables, including 2-points and 3-points goals and attempts, field goals and field goals attempts, free throws and free throws attempts, assists, blocks, steals, and total and offensive rebounds, had a right-skewed distribution, meaning that as the performance was getting better, the number of players was decreasing.
This more or less follows the Pareto Distribution which is to be expected in variables that represent human skill.

In contrast, Games had a left-skewed distribution with most players playing in around 35 games in a given year, while Games Started was a bimodal distribution, with a substantial number of players not forming the starting lineup in a given year.

Personal Fouls, with the exception of several outliers on the higher end, approximated a normal distribution with most players engaging in between 25 and 75 fouls in a given year.

Interestingly, the distribution of salary was bimodal with the two peaks being strikingly higher than the rest of the values which had a relatively high spread, from 50k to 250k.
This suggests that the salary doesn't reflect the performance stats directly and that other, perhaps not available variables, might play a role.



```{r}

# Check the more precise values of salaries distribution
wnba_complete %>% 
  mutate(
    salary_cat = cut(salary_adj, breaks = 30)
  ) %>% 
  count(salary_cat) %>% 
  arrange(desc(n))



```


Most of the salaries were between 131k and 144k or 70k and 84k.




```{r}

# Create a function for moving the year variable back to its original date
year_move <- function(x, factor = TRUE) {
  
  # Move the variable one value up
  year <- as.double(as.character(x)) + 1
  
  # Turn the variable into a factor
  if (factor) {year <- as.factor(year)}
  
  return(year)
  
  }

wnba_complete %>% 
  mutate(year = year_move(year)) %>%  # Move salaries back to their true date
  ggplot(aes(salary_adj)) +
  geom_histogram() +
  facet_wrap(~year)

```
Until 2020, none of the players earned more than 155k dollars. 
Since then, the salaries of a noticeable number of players have risen to between 155k and 255k dollars.
However, the number of players with salaries at the lower end (around 75k - 80k) has also grown, making them the majority.




```{r eval=FALSE}

pivoted_everything %>% 
  ggplot(aes(value)) +
  geom_boxplot() +
  facet_wrap(~name, scales = "free")


```


### Player's position numbers


```{r}

# Count player position numbers
positions <- wnba_complete %>% 
  freq_table(Pos) 



positions %>% 
  ggplot(aes(Pos, prop)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = str_c("n = ", n), vjust = -0.5))


```

Majority of the players played either on the G or F positions.
A still substantial number of players played on the C position.
There were few players who played on a mixed position in a given year.





## Salary over other variables


### Salary before and after adjustment

```{r}

# Obtain means, medians, and percentage change of both, for salaries
salaries_stats <- wnba_combined2 %>% 
  select(year, contains("salary")) %>% 
  group_by(year) %>% 
  get_summary_stats() %>% 
  select(year, variable, mean, median) %>% 
  group_by(variable) %>% 
  mutate(
    mean_change = (mean - lag(mean)) / lag(mean),
    median_change = (median - lag(median)) / lag(median),
      ) %>% 
  mutate(across(c(mean_change, median_change), ~replace_na(., 0))) %>% 
  ungroup() %>% 
  mutate(year = year_move(year))
  

# Plot absolute changes in salaries
salaries_stats %>% 
  pivot_longer(c(mean, median)) %>% 
  ggplot(aes(year, value, color = variable, group = variable)) +
  geom_point() +
  geom_line() +
  facet_wrap(~name)

```


The absolute change in salaries over the years is higher for salaries before the adjustment for inflation, which is especially visible since 2020.
Interestingly, the mean absolute change from 2021 to 2022 was negative for the adjusted salary.



```{r}

# Plot percentage changes for both statistics
salaries_stats %>% 
  pivot_longer(c(mean_change, median_change)) %>% 
  ggplot(aes(year, value, color = variable, group = variable)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  facet_wrap(~name)


```

The difference in the rate of change between adjusted and unadjusted values of salaries is even more clear when presented in the percentage change.
However, even after the adjustment, we can see that the salaries of WNBA players grew, with an exception around 2021. This could explained by the difficulties in organizing matches and gathering larger crowds that the sport organizations faced due to COVID-19 pandemic.



### Salary throughout years


```{r}

wnba_complete %>% 
  ggplot(aes(year_move(year), salary_adj)) +
  geom_boxplot()


```

Starting with 2020, a quarter of the players were earning more than $150,000 after adjusting for inflation, which was the maximum salary before that year.

Since 2021, the spread of the salaries has appeared to stabilize.




### Salary based on the position



```{r}


# Obtain position frequencies to place them on the boxplot
positions_median <- wnba_complete %>% 
  group_by(Pos) %>% 
  get_summary_stats(salary_adj, type = "median") %>% 
  mutate(
    salary_adj = median - 6000,
    n = str_c("n = ", n)
    )


wnba_complete %>% 
  ggplot(aes(Pos, salary_adj)) +
  geom_boxplot() +
  geom_text(aes(label = n), data = positions_median)


```



F-C position had the highest proportion of high-earning players, with half earning more than $200,000. This was closely followed by the G-F position, where salaries were slightly smaller. However, in both cases, the sample of players was small and, therefore, more prone to random fluctuations, making their estimates less reliable.

There were no substantial differences between the three unmixed positions (C, F, and G). Given that only these had relatively large sample sizes, this suggests that the position does not have a significant effect on the players' earnings.

### Salary based on WNBA stats


```{r fig.height=height_faceted}

# Pivot data for the 
pivoted_Onsalary <- wnba_complete %>% 
  select(where(is.double)) %>% 
  pivot_longer(-salary_adj)


pivoted_Onsalary %>% 
  ggplot(aes(value, salary_adj)) +
  geom_hex() +
  geom_smooth(se = F,  alpha = 1/3, size = 0.75, color = "orange") +
  scale_fill_viridis_c(option = "C", direction = 1) +
  facet_wrap(~name, scales = "free", ncol = 4) +
  theme(legend.position = "top")



```

Higher number of goals, regardless of the category, were associated with higher salaries, until reaching a level after which further increases in goals did not seem to have an impact. 
The exception was the 3-Points Goals, where, on average, each additional goal seemed to contribute to the salary.

A similar trend was observed for less direct performance metrics, like number of Assists, Blocks, Steals, and Turnovers.
In some cases there was a reverse at the higher values of a predictor with a small drop in salary, but this was due to just a few outliers and as a marker of the general relationship  can be disregarded. 

Overall, the associations between salary and different performance stats seem weak. Only a few could be roughly approximated using a linear model (e.g. 3-Points Goals and number of Assists).
This suggests that the relationship between a player's salary and her contribution to games is complex, and using non-linear modelling, like one that relies on decision trees, would be better at capturing it.



### Associations between predictors


#### 



#### Correlations between continuous predictors



```{r}

library(ggcorrplot)

wnba_complete %>% 
  select(-c(salary_adj, year, Pos)) %>% 
  cor(use = "pairwise.complete.obs", method = "spearman") %>% 
  ggcorrplot(
    type = "upper",
    insig = "blank"
    )
 
```

The majority of the continuous predictors were strongly and positively correlated with each other.

However, one performance statistic stood out - 3-point goals, for the most part, was either weakly or moderately correlated with the remaining predictors and with one (Offensive Rebounds), it was even negatively associated. 

Interestingly, the goal metrics, when expressed as a proportion of successes to attempts, had mostly weak associations and sometimes negative ones with the rest of the predictors.
This might be explained by their tendency to distort the true performance of the players who made only a few attempts in total.


```{r}

# Obtain a data frame with correlations between predictors
cor_predictors <- wnba_complete %>% 
  select(-c(salary_adj, year, Pos)) %>% 
  cor_test(method = "pearson") %>% 
  filter(cor != 1) %>% 
  arrange(desc(abs(cor))) %>% 
  mutate(across(where(is.numeric), ~round(., 3))) %>% 
  select(-c(statistic, method))

cor_predictors %>% 
  kable2()

```

There are a substantial number of highly correlated predictors.
First and foremost, we see near perfect correlations between successes and attempts for each performance category pair.
Similarly, there are very strong to near perfect correlations between number of points and some of the performance metrics, specifically, 2-point field goals, field goals, and free throws.




```{r}

# Filter through only the goal performance metrics 
cor_predictors %>% 
  filter(var1 %in% pct_filter, var2 %in% pct_filter  ) %>% 
  kable2()



```

Field goals, 2-point goals, and free throws were very strongly correlated with one another.


```{r}

# Obtain the number of strong correlations for each predictor separately
cor_predictors %>% 
  filter(abs(cor) >= 0.8) %>% 
  count(var1, name = "High correlations count")  %>% 
  arrange(desc(`High correlations count`)) %>% 
  kable2()

```

Field Goals, Field Goals Attempts and Points were the predictors with the highest count of strong correlations (|cor| > 0.8), posing the highest risk for multicollinearity when fitting linear models.



#### Position and other variables



```{r}

plan(multisession)

# Compare Positions in terms of continuous predictors using ANOVA 
pos_anova <- wnba_complete %>% 
  select(-c(year, salary_adj)) %>% 
  pivot_longer(-Pos) %>% 
  nest(.by = name) %>% 
  mutate(
    future_map_dfr(data, ~as_tibble(anova_test(., value ~ Pos))),
    significant = p <=  0.05
  )


plan(sequential)

```



```{r}

pos_anova %>% 
  ggplot(aes(ges, fct_reorder(name, ges), color = significant, alpha = significant)) +
  geom_point() +
  geom_hline(
    yintercept = sum(!pos_anova$significant) + 0.5, 
    linetype = "dashed"
    ) +
  labs(x = "Effect size (eta-squared)", y = "") +
  scale_alpha_discrete(range = c(0.4, 1))



```

There were `r sum(pos_anova$significant)` 13 continuous predictors which were significantly affected by Position, out of which in 
`r sum(pos_anova$ges >= 0.14)` cases the effect could be deemed large.

The strongest differences were observed for Offensive Rebounds and Blocks which was closely followed by percentage of Field Goals, 3-Points Goal Attempts, Total Rebounds and 3-Points Goals. 

There were no differences in non-performance stats, like Number of Games and Minutes played and, somewhat surprisingly, total number of Points and Field Goals.



```{r}

# Extract names of the outcomes for which the p was significant
pos_sig <- pos_anova %>% 
  filter(p <= 0.05) %>% 
  pull(name)

# Plot the comparisons of Positions in terms of the significant variables
wnba_complete %>%
  select(-c(salary_adj, year)) %>% 
  select(Pos, all_of(pos_sig)) %>% 
  pivot_longer(-Pos) %>% 
  left_join(pos_anova) %>% 
  ggplot(aes(Pos, value)) +
  geom_boxplot() +
  facet_wrap(~fct_reorder(name, ges, .desc = T), scales = "free", ncol = 3)


```


Center and Center-Forward positions were associated with higher numbers of Offensive Rebounds, Blocks and - to a smaller degree - Field Goal percentage, as well as lower numbers of 3-point Goals and Attempts in comparison to the remaining positions.

The highest numbers of assists were observed for the Forward-Center, Forward-Guard, and Guard positions. Forward-Guard and Guard also had the highest numbers of 3-point Goals.
On the other hand, these two and also the Guard-Forward position, had the smallest numbers of Total Rebounds.





# Modelling




## Features modification


```{r}
# Rename variables to make  them compatible with all modelling functions
wnba_complete <-  wnba_complete %>% 
  rename_with(~str_replace_all(., "2", "Two_")) %>% 
  rename_with(~str_replace_all(., "3", "Three_")) %>% 
  rename_with(~str_replace_all(., "%", "_pct")) %>% 
  mutate(Pos = str_replace(Pos, "-", "_") %>% factor())


# Turn factors into a dummy variable format
model.matrix(~ ., data = wnba_complete) %>% 
  as_tibble() %>% 
  select(-1)
```


## General setup 



### General parameters 

```{r}

# Extract values from parameters
log_transform <- params$log_transform # Add datasets with log transformed outcome
arrange_metric <- params$arrange_by # Performance metric by which the models should be arranged

# Arrange models by the metric of choice
arrange_own <- function(data) {
  if (arrange_metric == "Rsquared") {
    arrange(data, desc(Rsquared))
  } else {arrange(data, pick(arrange_metric))}
}


```



### Data separation


```{r}

# Create two versions of datasets
model_full <- wnba_complete %>% 
  mutate(across(ends_with("pct"), ~replace_na(., 0))) 

model_reduced <- wnba_complete %>% 
  select(-ends_with("pct"))

model_datasets <- tibble(
  dataset = c("full", "reduced"),
  whole = list(model_full, model_reduced)
)


```


```{r eval=TRUE}

model_datasets <-  model_datasets %>% 
  rename(original = whole ) %>% 
  mutate(log = map(original, function(x) {mutate(x, salary_adj = log(salary_adj))})) %>% 
  pivot_longer(-dataset, names_to = "transformed", values_to = "whole") %>% 
  mutate(
    dataset = str_c(dataset, "_", transformed),
    dataset = factor(dataset, levels = c(
      "full_original", 
      "reduced_original", 
      "full_log", 
      "reduced_log"
    ))
    ) %>% 
  select(-transformed) 



```


```{r}



# Set seed
set.seed(123)


# Set up k-indexing
indices <- sample(1:2, nrow(wnba_complete), prob = c(0.8, 0.2), replace = T)

# Extract general formula
general_formula <- salary_adj ~ .

model_datasets <- model_datasets %>% 
  mutate(
    train = map(whole, ~.[indices == 1,]),
    test = map(whole, ~.[indices == 2,]),
    train_dummy = map(train, ~as_tibble(model.matrix(~ ., .x)[,-1])),
    test_dummy = map(test, ~as_tibble(model.matrix(~ ., .x)[,-1])),
    x_train = map(train, ~model.matrix(general_formula, .x )[, -1]),
    y_train = map(train, ~.x$salary_adj),
    x_test = map(test, ~model.matrix(general_formula, .x )[, -1]),
    y_test = map(test, ~.x$salary_adj),
  )







```




### Functions


```{r}

# Extract model and its performance metrics from a caret object
extract_caret <- function(model) {
   
  # Extract
  model_stats <- model$results %>% 
    select(-ends_with("SD")) %>% 
    as_tibble() %>% 
    mutate(fit = list(model$finalModel))
  
  # Return
  return(model_stats)
  
}


cross_fit <- function(x_data, y_data, ..., tuning, method = "glmnet") {
  
  # train cross-validation models
  model <- train(
    x_data, y_data, ...,
    method = method,
    tuneGrid = tuning,
    trControl = trainControl(method = "cv", number = 10)
  )
  
  
  # Return model and its performance metrics
  return(extract_caret(model))
  
}

```



## Linear models 

Linear models will be fitted using a combination of forward and backward stepwise selections.


```{r}
# Extract regsubsets function
regsubset_own <- function(data, nvmax, method = "forward") {
  regsubsets(
    data = data, 
    nvmax = nvmax, 
    x = general_formula, 
    method = method
    )
}




# Function for predicting values from regsubset object
subset_lm <- function(regsubset, id, data) {
  
  # Extract coefficients
  xvars <- coef(regsubset, id = id) %>% 
    names() %>% 
    keep(~. != "(Intercept)")
  
  
  # Create formula
  form <- str_c("salary_adj ~ ", str_c(xvars, collapse = " + ")) %>% 
    as.formula()
  
  # Set up the train control for 10-fold cross-validation
  train_control <- trainControl(method = "cv", number = 10)
  
  # Fit the linear model using the train function from the caret package
  model <- train(form, data = data, method = "lm", trControl = train_control)
  
  # Return model and its performance stats
  return(extract_caret(model))
  
  
}

# Set seed
set.seed(123)

regsubsets_models <- model_datasets %>% 
  select(dataset, train, train_dummy, test_dummy) %>% 
  mutate(
    id = map(train, ~1:(ncol(.) - 1)),
    predictors_total = map_dbl(train, ncol),
    fit_forward = map2(train, predictors_total, regsubset_own,method = "forward"),
    fit_backward = map2(train, predictors_total, regsubset_own,method = "backward")
    ) 
  

# Fit all best selected models 
regsubsets_models2 <- regsubsets_models %>% 
  select(-c(train,  test_dummy, predictors_total)) %>% 
  pivot_longer(c(fit_forward, fit_backward), names_to = "method", values_to =  "reg_model") %>% 
  unnest(id)  %>% 
  mutate(
    lm = pmap(list(reg_model, id, train_dummy), subset_lm),
  ) %>% 
  unnest(lm) %>% 
  mutate(
    across(where(is.double), ~round(., 3)),
    predictors = id - 1
    ) %>% 
  arrange_own()

# Select the best models
best_lm <- regsubsets_models2 %>% 
  group_by(dataset) %>% 
  slice(1) %>% 
  select(dataset, fit) %>% 
  ungroup()

# 10 best
regsubsets_best10 <- regsubsets_models2 %>% 
  select(dataset, predictors, method, RMSE, MAE, Rsquared) %>% 
  slice(1:10) 

regsubsets_best10 %>% 
  kable2() 

```

### Models' Ranking

#### Top 10 models

Cross-validation revealed that the best 10 linear models explained between 
`r min(regsubsets_best10$Rsquared)`
`r max(regsubsets_best10$Rsquared)` proportion of the total variance,
having between 
`r min(regsubsets_best10$predictors)` and
`r max(regsubsets_best10$predictors)` predictors.


#### Performance metrics Comparison


```{r}
# Plot scatterplots of number of variables against R-squared
regsubsets_models2 %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared)) %>%
  ggplot(aes(predictors, Rsquared, color = dataset, alpha = best)) +
  geom_point() +
  facet_wrap(~dataset)
```
The explanatory power of the linear models consistently increases up to around 10 predictors, after which the relationship plateaus.


```{r}
regsubsets_models2 %>% 
  select(dataset, predictors, RMSE, MAE) %>% 
  pivot_longer(c( RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(best = Performance == min(Performance)) %>% 
  ggplot(aes(predictors, Performance, color = dataset, alpha = best)) +
  geom_point() +
  facet_grid(dataset ~ Metric, scales = "free")

```



### Best models coefficients



```{r}
# Obtain coefficients, residuals and diagnostics
best_lm2 <- best_lm %>% 
  mutate(
    coefs = map(fit, ~lm.beta::lm.beta(.) %>% tidy()),
    augmented = map(fit, augment),
    vif = map(fit, vif),
    vif_names = map(vif, names)
    )

lm_coefs <- best_lm2 %>% 
  select(dataset, coefs) %>% 
  unnest() %>% 
  group_by(dataset) %>% 
  arrange(desc(std_estimate)) %>% 
  ungroup() %>% 
  mutate(across(where(is.double), ~round(., 3))) %>% 
  mutate(
    std_estimate2 = if_else(
      p.value <= 0.05, 
      str_c(std_estimate, "*"), 
      as.character(std_estimate) 
    ),
    significant = p.value <= 0.05
  )
```


```{r}
# Plot standardized coefficients of the predictors for the four best linear models together
lm_coefs %>% 
  ggplot(aes(std_estimate, term, color = dataset, alpha = significant)) +
  geom_point(position = position_jitter(width = 0, height = 0.3)) +
  coord_cartesian(xlim = c(
    -abs(max(lm_coefs$std_estimate, na.rm = T)),
     abs(max(lm_coefs$std_estimate, na.rm = T)))
    ) +
  geom_vline(xintercept = 0)

```

Regardless of the model, field goals, year, and number of games were the strongest predictors of a WNBA player's salary.

The more field goals a player had, the better her salary was. Interestingly, the reverse was true for the number of games. Salaries were visibly higher from 2019 to 2022 than in 2017 and 2018 with a stabilization being achieved beginning with 2019.

Additionally, the number of minutes on the field was an important predictor in models with the log-transformed outcome.


```{r}
# Show standardized coefficients for all models in a table
lm_coefs %>% 
  select(dataset, term, std_estimate2) %>% 
  pivot_wider(names_from = dataset, values_from = std_estimate2) %>% 
  kable2()
  


```



### Model diagnostics


#### Normality assumption


```{r}
best_lm3 <- best_lm2 %>% 
  unnest(augmented) %>% 
  select(-fit)

# Plot histograms
best_lm3 %>% 
  ggplot(aes(.resid)) +
  geom_histogram() +
  facet_wrap(~dataset, scales = "free")
```


```{r}
# Plot Q-Q plots
best_lm3 %>% 
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~dataset, scales = "free")
  



```

Judging by histograms and Q-Q plots, a slight deviation from normality can be observed, with more observations at the tails. However, this slight deviation should not significantly affect the estimation of the coefficients."

#### Homoscedasticity assumption



```{r}

best_lm3 %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~dataset, scales = "free")
  

```

The models fitted to the untransformed data suffered from heteroscedasticity, with a lower error variance at the smaller predicted values. 
This was somewhat, but not entirely, corrected in the models with the log-transformed outcome.
Moreover, all models displayed non-linearity.


#### Multicollinearity




```{r}
# Plot VIF 
best_lm2 %>% 
  select(dataset, vif, vif_names) %>% 
  unnest() %>% 
  ggplot(aes(vif, vif_names, color = dataset)) +
  geom_point(position = position_jitter(height = 0.3))

```

Each of the four best models reported extremely high correlations for the same two predictors:
-   FG (Field goals)
-   FGA (Field goals attempts)

This is in line with the correlation analyses, which revealed that the two predictors had the highest count of correlation above |0.8| with the remaining predictors.

Additionally, models fitted to the datasets with the logarithmitized outcome, reported a high correlation for Minutes, while models fitted to the datasets with the untransformed outcome had a high correlation for TRB (Total Rebounds).


#### Outliers and influential values

##### Cook's distance

```{r}

# Add conditional variables indicating influential and outlier values 
best_lm3 <- best_lm3 %>% 
  mutate(Cook_high = if_else(.cooksd >= 1, T, F)) 

# Plot cook's distance values
best_lm3 %>% 
  ggplot(aes(dataset, .cooksd)) +
  geom_jitter()
  
  

```

No influential values were observed.


##### Outliers

```{r}

# Plot standardized residuals
best_lm3 %>% 
  ggplot(aes(dataset, .std.resid)) +
  geom_jitter()


```

No observations with a standardized residual greater than |x| > 3 were found.


#### Summary 

Even though the best linear models explained a substantial proportion of the outcome's variance, they suffered from moderate heteroscedasticity and, even more importantly, they displayed non-linearity and some of their predictors had a very high VIF. 
All this means that the estimated coefficients are not reliable making the models less trust-worthy.


## Lasso models


```{r}
# Set seed
set.seed(123)



# Create search grid for lambda parameter in lasso
tune_lasso <- data.frame(
  alpha = 1,
  lambda = 10^seq(3, -2, length = 100) 
)


# Start parallel computing
cl <- makeCluster(num_cores)
registerDoParallel(cl)


# Fit lasso models
lasso_models <- model_datasets %>% 
  select(dataset, x_train, y_train) %>% 
  mutate(
    fit_lasso = map2(x_train, y_train, cross_fit, tuning = tune_lasso),
    ) %>% 
  unnest(fit_lasso) %>% 
  arrange_own() 

# Stop parallel computing
stopCluster(cl)
registerDoSEQ()

# Extract best models
best_lasso <- lasso_models %>% 
  group_by(dataset) %>% 
  slice(1) %>% 
  select(dataset, lambda, fit) %>% 
  ungroup()


# Extract performance metrics information 
lasso_models <- lasso_models %>% 
  select(-c(x_train, y_train, alpha, fit)) 

```

### Inspect warnings



```{r}
# Inspect performance metrics statistics for unusual values
lasso_models %>% 
  group_by(dataset) %>% 
  get_summary_stats(-lambda) %>% 
  kable2()

```

A high number of models fitted to the transformed datasets lack R-square estimation.


```{r}

# Summarize only the models with missing R-squared values
lasso_models %>% 
  filter(is.na(Rsquared)) %>% 
  get_summary_stats() %>% 
  kable2()


```

As seen in the table above, the lack of R-squared estimation concerned models with higher lambda values.
This is probably associated with the nature of the outcome after log-transformation which largely shrank its original value. 



### Models' ranking

#### Top 10 models


```{r}

# Extract the best 10 models
best_lasso10 <- lasso_models %>% 
  slice(1:10)

# Arrange lasso models in terms of the chosen performance metric
lasso_models %>% 
  kable2()



```



Cross-validation revealed that the best 10 linear models explained between 
`r min(best_lasso10$Rsquared)`
`r max(best_lasso10$Rsquared)` proportion of the total variance,
with lambda values being between
`r min(best_lasso10$lambda)` and
`r max(best_lasso10$lambda)` predictors.





#### Performance metrics Comparison


```{r}
# Plot scatterplots of number of variables against R-squared
lasso_models  %>% 
  group_by(dataset) %>% 
  mutate(
    best = Rsquared == max(Rsquared, na.rm = T),
    lambda = log(lambda)
    ) %>%
  ungroup() %>% 
  drop_na(Rsquared) %>% 
  ggplot(aes(lambda, Rsquared, color = dataset, alpha = best, shape = best)) +
  geom_point() +
  facet_wrap(~dataset)
```

Models fitted to the untrasformed datasets resulted in similar performance across all lambda values with models being slighlty better at the higher lambda values. 
The latter was somewhat more pronounced in the case of the reduced dataset.

In contrast, 
lasso models fitted to the datasets with log-transformed outcome performed better at smaller values of lambda with a noticeable and consistent decrease starting with a `log(lambda)` value of `-4`.


```{r}
lasso_models %>% 
  drop_na(Rsquared) %>% 
  select(dataset, lambda, RMSE, MAE) %>% 
  pivot_longer(c( RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(
    best = Performance == min(Performance),
    lambda = log(lambda)
    ) %>% 
  ungroup() %>% 
  ggplot(aes(lambda, Performance, color = dataset, alpha = best, shape = best)) +
  geom_point() +
  facet_grid(dataset ~ Metric, scales = "free")

```

The trends observed for R-squared comparison are reflected in the comparison of the models in terms of RMSE and MAE.
The models fitted to the log-transformed data achieved best predictions for smaller values of lambda, while the models fitted to the untransformed data displayed a better performance for higher values of lambda.


### Best models' coefficients


```{r}

# extract coefficients of only the best lasso models
best_lasso <- best_lasso %>% 
  mutate(
    coefs = map2(fit, lambda, ~coef(.x, s = .y)),
    coefs = map(coefs, ~as.data.frame.matrix(.) %>% rownames_to_column() )
    ) 

# Fit linear models using only those predictors which coefficients weren't reduced to zero
best_lasso_reduced <- best_lasso %>% 
  unnest(coefs) %>% 
  filter(s1 != 0, rowname != "(Intercept)") %>% 
  select(dataset, rowname) %>% 
  nest(.by = dataset) %>% 
  mutate(
    x_vars = map_chr(data, ~str_c(unlist(.), collapse = " + ")),
    formula = str_c("salary_adj ~ ", x_vars),
    formula = map(formula, as.formula)
    ) %>% 
  select(dataset, formula) %>% 
  left_join(model_datasets)  %>% 
  ungroup() %>% 
  mutate(
    fit = map2(formula, train_dummy, ~lm(.x, .y))
  ) %>% 
  select(dataset, test_dummy, fit) 


# Obtain model diagnostic statistics and performance metrics
best_lasso_reduced2 <- best_lasso_reduced %>% 
  mutate(
    RMSE = map2_dbl(fit, test_dummy,  rmse),
    Rsquared = map2_dbl(fit, test_dummy,  rsquare),
    MAE = map2_dbl(fit, test_dummy,  mae),
    map_df(fit, glance),
    coefs = map(fit, ~lm.beta::lm.beta(.) %>% tidy()),
    vif = map(fit, vif),
    vif_names = map(vif, names),
    augmented = map(fit, augment)
    ) %>% 
  arrange_own()
  
  


# Extract coefficients from the updated best lasso models after reduction of zero-coefficient predictors 
lasso_coefs <- best_lasso_reduced2 %>% 
  select(dataset, coefs) %>% 
  unnest() %>% 
  group_by(dataset) %>% 
  arrange(desc(std_estimate)) %>% 
  ungroup() %>% 
  mutate(across(where(is.double), ~round(., 3))) %>% 
  mutate(
    std_estimate2 = if_else(
      p.value <= 0.05, 
      str_c(std_estimate, "*"), 
      as.character(std_estimate) 
    ),
    significant = p.value <= 0.05
  )




```



```{r}
# Plot standardized coefficients of the predictors for the four best linear models together
lasso_coefs %>% 
  ggplot(aes(std_estimate, term, color = dataset, alpha = significant)) +
  geom_point(position = position_jitter(width = 0, height = 0.3)) +
  coord_cartesian(xlim = c(
    -abs(max(lasso_coefs$std_estimate, na.rm = T)),
     abs(max(lasso_coefs$std_estimate, na.rm = T)))
    ) +
  geom_vline(xintercept = 0)

```


### Model diagnostics


```{r}

# Extract augmented statistics
best_lasso_augmented <- best_lasso_reduced2 %>% 
  select(dataset, augmented) %>% 
  unnest(augmented)

```


#### Normality assumption

##### Histograms

```{r}
# Plot the histograms of the models' residuals
best_lasso_augmented %>% 
  ggplot(aes(.std.resid)) +
  geom_histogram() +
  facet_wrap(~dataset)


```

The histograms indicate that the distributions of the residuals of all models roughly conformed to the normal distribution.


##### Q-Q plots


```{r}
# Obtain Q-Q plots of the models' residuals
best_lasso_augmented %>% 
  ggplot(aes(sample = .std.resid)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~dataset)


```


Based on the Q-Q plots, it can be observed that the models fitted on the untransformed salary variable did not deviate significantly from the normal distribution.

The case was somewhat different for the models fitted on the logarithmically transformed dependent variable, where moderate deviations were observed at the edges.


#### Homoscedasticity assumption


```{r}

# Plot scatterplots of the predicted values against the residuals
best_lasso_augmented %>% 
  ggplot(aes(.fitted, .resid)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_wrap(~dataset, scales = "free")

```

All models suffered from heteroscedasticity, although this was less pronounced in the case of the models fitted to the log-transformed dataset.

#### Multicollinearity

```{r}
best_lasso_vif <- best_lasso_reduced2 %>% 
  select(dataset, vif, vif_names) %>% 
  unnest() 



# Plot VIF of the best lasso models
best_lasso_vif %>% 
  unnest() %>% 
  ggplot(aes(vif, vif_names, color = dataset)) +
  geom_point(position = position_jitter(height = 0.3))

```

Assuming the criterion of VIF > 4, all models were reported to have some degree of multicollinearity, with the field goals predictor achieving the highest multicollienarity in each of the models.



#### Outliers and influential values

##### Cook's distance

```{r}

# Add conditional variables indicating influential and outlier values 
best_lasso_augmented <- best_lasso_augmented %>% 
  mutate(Cook_high = if_else(.cooksd >= 1, T, F)) 

# Plot cook's distance values
best_lasso_augmented %>% 
  ggplot(aes(dataset, .cooksd)) +
  geom_jitter()
  
  

```

No influential values were observed.


##### Outliers

```{r}

# Plot standardized residuals
best_lasso_augmented %>% 
  ggplot(aes(dataset, .std.resid)) +
  geom_jitter()


```

No observations with a standardized residual greater than |x| > 3 were found.


#### Summary 

The linear models obtained with the lasso shrinkage method achieved better performance in terms of the assumptions of linear regression. 
Multicollinearity, overall, was substantially lower, and the distribution of residuals, at least in the case of the models fitted to the untransformed data, better approximated a normal distribution. 
However, their predictive performance as measured by cross-validation was somewhat lower.

## Tree models


```{r}
# Set seed
set.seed(123)


# Set up tuning grid for tree models
tune_tree <- data.frame(cp = seq(0.001, 0.1, 0.001) )




# Obtain tree models
tree_models <- model_datasets %>% 
  select(dataset, x_train, y_train) %>% 
  mutate(
    fit_tree = map2(
      x_train, 
      y_train, 
      cross_fit, 
      tuning = tune_tree,
      method = "rpart"
      ))



# Arrange models in terms of RMSE
tree_models2 <- tree_models %>% 
  select(-contains("train")) %>% 
  unnest(fit_tree) %>% 
  arrange(RMSE)

# Extract best tree models
best_tree <- tree_models2 %>% 
  group_by(dataset) %>% 
  slice(1) %>% 
  select(dataset, fit) %>% 
  ungroup()
```


### Models' ranking

```{r}
# Arrange tree models from best to worst
tree_models2 %>% 
  select(-fit) %>% 
  arrange_own() %>% 
  kable2()

```



#### Performance Metrics Comparison


```{r}



# Plot R-squared of the models
tree_models2 %>% 
  select(-fit) %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared, na.rm = T)) %>% 
  ungroup() %>% 
  ggplot(aes(cp, Rsquared, color = dataset, alpha = best)) +
  geom_point() +
  facet_wrap(~dataset, scales = "free")
```

Overall, regardless of the dataset category, smaller values of complexity parameter were associated with a better model fit.
Therefore, allowing for more complex tree models with less pruning resulted in better predictions of the salary.



```{r}

tree_models2 %>% 
  select(-fit) %>% 
  pivot_longer(
    cols = c(RMSE, MAE),
    values_to = "Performance",
    names_to = "Metric"
    ) %>% 
  group_by(dataset, Metric) %>% 
  mutate(best = Performance == min(Performance, na.rm = T)) %>% 
  ungroup() %>% 
  ggplot(aes(cp, Performance, color = dataset, alpha = best, shape = best)) +
  geom_point() +
  facet_grid(dataset ~ Metric, scales = "free")

```

Judging by RMSE, models achieved the best fit when the complexity was at a level between 0.025 and 0.05.

Analysis of the MAE results indicates a stronger difference in model optimization between different datasets, with the models fitted to the datasets with a log-transformed outcome showing preference for higher cp values, above around 0.04.


### Predictors' analysis the best models


```{r}

# Function to extract predictors and their importance from the final tree
rpart_predictors <- function(fit) {
  
  # Extract names of the fitted predictors
  predictors <- fit$frame$var %>% 
    keep(~. != "<leaf>")
  
  fit$variable.importance %>% 
    keep_at(~. %in% predictors)
  
  
}

# Plot predictors importance
best_tree %>% 
  mutate(
    importance = map(fit, rpart_predictors),
    importance_names = map(importance, names)
  ) %>%  
  unnest(cols = -fit) %>% 
  ggplot(aes(y = importance_names, x = importance, color = dataset)) +
  geom_point(position = position_jitter(width = 0.2, height = 0.2))


```


Number of Games Started was the only predictor that was used as a split in all four tree models and it was also associated with the highest importance in the models fitted to the untransformed datasets.






### Tree leafs plots


```{r}
# Move best tree fits into a list
best_trees_list <- best_tree$fit %>% 
  set_names(best_tree$dataset)
```


#### Full untransformed dataset


```{r}

rpart.plot(best_trees_list$full_original, digits = -3)

```


The optimal tree model among those fitted to the full untrasfromed dataset predicted the highest salary for WNBA players who had at least 11 Games Started, more than 0.5 Two Point Field Goals percentage and played in 2018. 
Having lass than 11 Games started was associated with the lowest salary.

#### Reduced untransformed dataset

```{r}


rpart.plot(best_trees_list$reduced_original, digits = -3)

```

The best among models fitted to the reduced untransformed dataset revealed that only number of Games Started was predictive of salary with players having at least 11 Games Started having a higher salary.



#### Full log-transformed dataset


```{r}

rpart.plot(best_trees_list$full_log, digits = -5)

```


#### Reduced log-transformed dataset


```{r}

rpart.plot(best_trees_list$reduced_log, digits = -5)

```

Similar to the reduced untransformed dataset, the optimal model fitted to the reduced dataset with a log-transformed outcome revealed only one predictor: players with at least 11 games started were associated with a higher salary.

### Summary

Comparison of tree models showed a lack of conformity between the three performance measures, making it more difficult to choose the best model for each dataset and, consequently, to compare them further with other model types. The models were also characterized by a relatively small number of selected predictors, which could stem from very high multicollinearity among most variables. Overall, the tree models performed noticeably worse compared to the linear models.

The models were also characterized by a relatively small number of selected predictors which could stem from very high multicollinearity among most variables. 

Overall, the tree models performed noticeably worse in comparison to the linear models. 






## Random forest


```{r}
# Tuning grid for random forest
tuning_forest <- data.frame(mtry = seq(2, ncol(model_full), 2))
```


```{r eval=params$evaluate_chunks}
# Set seed
set.seed(123)


# Start parallel computing
cl <- makeCluster(num_cores)
registerDoParallel(cl)




model_datasets$x_train[[1]]

# Fit random forest models
forest_models <- model_datasets %>% 
  expand_grid(ntree = c(100, 150, 200, 250, 300, 400, 500, 800, 1000)) %>% 
  select(dataset, x_train, y_train, ntree) %>% 
  mutate(fit_forest = pmap(
    list(x_data = x_train, y_data = y_train, ... = ntree), 
    cross_fit, 
    tuning = tuning_forest, 
    method = "rf")
    )

# forest_models2 <- forest_models 

# Stop parallel computing
stopCluster(cl)
registerDoSEQ()
```


```{r eval=params$evaluate_chunks}
# Export random forest object to avoid long processing time
save(forest_models, file = "forest_models.RData")

```



```{r}

load("forest_models.RData")




```

### Random forest models ranking

```{r}
# Extract best forest models for each dataset type
best_forest <- forest_models %>% 
  unnest(fit_forest) %>% 
  group_by(dataset) %>% 
  arrange_own() %>% 
  select(dataset, fit) %>% 
  slice(1) %>% 
  ungroup()

```




```{r}

# Extract model performance metrics by ntree and mtry
forest_fitted <- forest_models %>% 
  unnest(fit_forest) %>% 
  select(dataset, ntree, mtry, RMSE, Rsquared, MAE) %>% 
  arrange_own() 

# Best 10 forest models
best_forest10 <- forest_fitted %>% 
  slice(1:10)

# Show all models
forest_fitted %>% 
  kable2()

```

#### Top 10 models

Cross-validation revealed that the best 10 random forest models explained between 
`r min(best_forest10$Rsquared)`
`r max(best_forest10$Rsquared)` proportion of the total variance,
with mtry being between
`r min(best_forest10$mtry)` and
`r max(best_forest10$mtry)` predictors.

Except for one model, the preferable number of trees for growing was between 100 and 150.

The analysis also revealed that the best models were those fitted either to the reduced untransformed data or the full dataset with a log-transformed outcome.




#### R-squared Comparison

```{r}

# Add information about best fit for each model type based on R2
forest_rsquared <- forest_fitted %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared)) %>% 
  ungroup() 


# Set common options for heatmaps
heatmap_aesthetics <- list(
  scale_fill_viridis_c(option = "D", direction = 1)
)

# Set common options for heatmaps
heatmap_aesthetics2 <- list(
  scale_fill_viridis_c(option = "D", direction = -1)
)




# Compare tuning parameters in terms of the R-squared on a heatmap
forest_rsquared %>% 
  ggplot(aes(factor(mtry), factor(ntree))) +
  geom_tile(aes(fill = Rsquared)) +
  geom_point(data = filter(forest_rsquared, best), color = dot_color) +
  facet_wrap(~dataset, scales = "free") +
  labs(y = "Number of trees to grow",x = "Number of variables randomly sampled") +
  theme(legend.position = "top") +
  heatmap_aesthetics

```


Overall, models with higher `mtry` values were associated with a higher R-squared. 
A more pronounced difference was observed for the number of trees parameter, with a higher number of trees leading to better performance in the full untransformed dataset, while, in contrast, a smaller number of trees was more effective for the reduced untransformed dataset.

#### RMSE and MAE comparison

```{r}

# Add information about best fit for each model type based on R2
forest_performance <- forest_fitted %>% 
  pivot_longer(cols = c(RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(
    best = Performance == min(Performance),
    transformed = if_else(str_detect(dataset, "log"), "Untransformed", "Transformed")
    ) %>% 
  ungroup() %>% 
  nest(.by = c(Metric, transformed)) 
  




# Create list for all plots
plots_forest <- vector("list", nrow(forest_performance))


# Creat heatmaps for each metric and separately for transformed and untrasformed datasets
for (i in 1:nrow(forest_performance)) {
  
  plots_forest[[i]] <-  forest_performance$data[[i]] %>% 
    # filter(str_detect(dataset, "original"), Metric == "RMSE") %>% 
    ggplot(aes(factor(mtry), factor(ntree))) +
    geom_tile(aes(fill = Performance)) +
    geom_point(data = filter(forest_performance$data[[i]], best), color = "red") +
    facet_wrap(~dataset, scales = "free") +
    labs(
      y = "Number of trees to grow",
      x = "Number of variables randomly sampled",
      title = str_c(
        "Comparison of ", forest_performance$transformed[[i]], " models in terms of ", forest_performance$Metric[[i]]
        )
      ) +
    heatmap_aesthetics2
}

```



```{r}
# Compare models in terms of RMSE 
cowplot::plot_grid(plots_forest[[1]], plots_forest[[3]], nrow = 2)
```

When compared in terms of RMSE and MAE, random forest models fitted to the untransformed datasets revealed a similar pattern of tuning parameter preference to the R-squared comparison.
However, the analysis of mean absolute error identified a different optimal model for the reduced untransformed dataset, which was associated with a higher `mtry` value.


```{r}
# Compare models in terms of MAE
cowplot::plot_grid(plots_forest[[2]], plots_forest[[4]], nrow = 2)

```
The pattern of tuning parameter preference for the transformed dataset model fitting was very similar across all three performance metrics, with the same tuning parameters identified for the optimal models.

### Predictors importance


```{r}

# Extract predictors' importance for each model
best_forest2 <- best_forest %>% 
  mutate(
    importance_purity = map(fit, ~importance(.) %>% as.data.frame() %>% rownames_to_column())
    )


```




```{r}
# Plot predictors importance values for the untransformed datasets
best_forest2 %>% 
  select(dataset, importance_purity) %>% 
  unnest() %>% 
  filter(str_detect(dataset, "original")) %>% 
  ggplot(aes(y = rowname, x = IncNodePurity, color = dataset)) +
  geom_point()
```
Interestingly, the random forest models revealed `Games` and `Games Started` as the most important predictors.
Less important, but still noticeable predictors, were `Field Goals`, `Personal Fouls`, `Points`, and `2-Point Field Goals`.





```{r}
# Plot predictors importance values for the transformed datasets
best_forest2 %>% 
  select(dataset, importance_purity) %>% 
  unnest() %>% 
  filter(str_detect(dataset, "log")) %>% 
  ggplot(aes(y = rowname, x = IncNodePurity, color = dataset)) +
  geom_point() 

```


#### Partial Dependence Plots


```{r include=FALSE}
# Create a function for extracting y and x values from a partial plot object
extract_partial <- function(partial_data) {

  
  
  # Return the partial data as a data frame
  tibble(
    predictor = partial_data$x, 
    salary = partial_data$y
    )
}



```


```{r include=FALSE}

# Enable parallel computing
plan(multisession)


# Extract partial predictions for most important predictors
best_forest2 <- best_forest %>% 
  left_join(model_datasets) %>% 
  mutate(
    plots_Games = future_map2(fit, x_train, partialPlot, x.var = "Games", plot = FALSE),
    plots_GS = future_map2(fit, x_train, partialPlot, x.var = "GS", plot = FALSE),
    plots_FG = future_map2(fit, x_train, partialPlot, x.var = "FG", plot = FALSE),
    plots_PTS = future_map2(fit, x_train, partialPlot, x.var = "PTS", plot = FALSE),
    plots_PF = future_map2(fit, x_train, partialPlot, x.var = "PF", plot = FALSE),
    plots_Two_P = future_map2(fit, x_train, partialPlot, x.var = "Two_P", plot = FALSE),
  ) %>% 
  select(dataset, starts_with("plots")) %>% 
  pivot_longer(-dataset, names_to = "predictor_name", values_to = "partial_predictions") %>% 
  mutate(
    predictor_name = str_replace(predictor_name, "plots_", ""),
    partial_predictions = future_map(partial_predictions, extract_partial)
    ) 

plan(sequential)

```



```{r }


# Plot partial predictions of the selected variables
best_forest2 %>% 
  unnest() %>% 
  mutate(salary = if_else(str_detect(dataset, "log"), exp(salary), salary)) %>% 
  ggplot(aes(predictor, salary, color = dataset, group = dataset)) +
  geom_line() +
  facet_wrap(~predictor_name, scales = "free") +
  theme(legend.position = "top")


```

Across all datasets, Field Goals, Games Started, Points, and Two Points Field Goals were positively associated with Salary, while number of Games and Personal Fouls were negative predictors of Salary.


Salary started to increase largely with more than 50 Field Goals with the increase levelling at around 150 Field Goals. Similar trends were observed for for Games Started for numbers between 10 and 30 and, not as sharp, Points for values between 200 and 500. 

The increase in salary in response to increase in number of Two Points Field Goals was more drastic, with a high increase of the former between 75 and 90 Two Points Field Goals with values above 100 up to
`r max(wnba_complete$Two_P)` showing no further improvement. 
This suggests that there is a narrow effect window strongly differenting players in terms of their salary.

Number of Games was associated with a steady decrease in salary until leveling at around a value of 40.

Personal Fouls displayed a more discrete relationship with Salary with values around 10 and then 45 marking two large drops in Salary.



### Summary

The random forest achieved better predictions than standard tree models in terms of cross-validation, but performed worse compared to the linear models. The pattern of the best predictors partly overlaps with the standard tree models, specifically concerning the Number of Games Started and two-point goals, as well as with the linear models, in terms of number of Field Goals and Games. 
In contrast to the linear models, year was deemed as one of the least important predictors.

Interestingly, the models fitted to the log-transformed and reduced dataset performed noticeably worse than the models from the remaining three sets.




## Gradient-boosting trees

```{r}
# Set seed
set.seed(123)



# Vary nrounds by eta
eta_nrounds <- tibble(
  eta = c(0.01, 0.05, 0.1, 0.2, 0.3),
  nrounds = list(
    c(500, 1000, 1500),
    c(200, 500, 800),
    c(100, 200, 400),
    c(50, 100, 200),
    c(30, 50, 100)
  )
)


# Tuning grid for boosted trees
tune_boosted <- eta_nrounds %>% 
  expand_grid(
      max_depth = seq(1, 9, 2),
      gamma = 0,
      colsample_bytree = 1,
      min_child_weight = 1,
      subsample = 1
      ) %>% 
  unnest()
  



  
  
# Define a quite version of the cross_fit function to capture warnings
quiet_cross_fit <- quietly(cross_fit)

# Set up parallel computing
cl <- makeCluster(num_cores)
registerDoParallel(cl)


# Train boosted tree models
boosted_models <- model_datasets %>% 
  select(dataset, x_train, y_train) %>% 
  mutate(
    fit_boost = map2(x_train, y_train, quiet_cross_fit, tuning = tune_boosted, method = "xgbTree")
  )


# Stop parallem computing
stopCluster(cl)
registerDoSEQ()
```



### Inspect warnings


```{r eval=FALSE}

# Extract strings with warning information
boosted_warnings <- boosted_models %>% 
  select(dataset, fit_boost) %>% 
  unnest(fit_boost) %>% 
  filter(map_lgl(fit_boost, is.character)) %>% 
  unnest(fit_boost) %>% 
  filter(fit_boost != "")

boosted_warnings %>% 
  kable2()

```




```{r }
# Extract boosted model results statistics
boosted_models <- boosted_models %>% 
  select(dataset, fit_boost) %>% 
  unnest(fit_boost) %>% 
  filter(map_lgl(fit_boost, is_tibble))


# Extract performance metrics for all boosted models
boosted_models2 <- boosted_models %>% 
  unnest(fit_boost) %>% 
  select(dataset, eta, max_depth, nrounds, RMSE, MAE, Rsquared) %>% 
  arrange_own()

# Extract best 10 boosted models for reference 
boosted_best10 <- boosted_models2 %>% 
  slice(1:10)

# Extract best boosted models
best_boosted <- boosted_models %>% 
  unnest(fit_boost) %>% 
  group_by(dataset) %>% 
  arrange_own() %>% 
  select(dataset, fit) %>% 
  slice(1) %>% 
  ungroup()
```


```{r eval=FALSE}
# Inspect performance metrics among the models that had missing values in resampled performance measures
boosted_models2 %>% 
  filter(str_detect(dataset, "log")) %>% 
  group_by(dataset) %>% 
  get_summary_stats(RMSE, MAE, Rsquared)


```




```{r eval=FALSE}
boosted_models2 %>% 
  filter(is.na(Rsquared) | Rsquared < 0.01)


```




### Models' ranking

#### Top 10 models

```{r}



# Show all boosted models in a table
boosted_models2 %>% 
  arrange_own() %>% 
  kable2() 

```

The ten gradient-boosted models with the best performance, as revealed by cross-validation, explained between 
`r min(boosted_best10$Rsquared)` and
`r max(boosted_best10$Rsquared)` proportion of the total variance, with most being those fitted to the datasets with a log-transformed outcome. 





#### R-squared Comparison

```{r}

# Combine eta and nround columns into one
boosted_models3 <- boosted_models2 %>% 
  mutate(
    eta_nround = interaction(eta, nrounds)
  ) %>% 
  filter(!(is.na(Rsquared) | Rsquared < 0.01))

# Add information about the optimal model based on R-squared
boosted_rsquared <- boosted_models3 %>% 
  group_by(dataset) %>% 
  mutate(best = Rsquared == max(Rsquared, na.rm = T)) %>% 
  ungroup() 

# Compare tuning parameters in terms of R-squared on a heatmap
boosted_rsquared %>% 
  ggplot(aes(factor(max_depth), eta_nround)) +
  geom_tile(aes(fill = Rsquared)) +
  geom_point(data = filter(boosted_rsquared, best), color = "red") +
  facet_wrap(~dataset, scales = "free") +
  labs(y = "Learning rate x boosting iterations",x = "Maximum depth of a tree") +
  heatmap_aesthetics


  
  
```


Each dataset followed a different pattern of preferable gradient-boosting parameters. When judged by R-squared, shallow trees generally produced better-performing models, with the possible exception of those fitted to the full log-transformed dataset, where tree depth was a less important factor. 
Additionally, the comparison of learning rate and number of iterations combinations did not reveal a noticeable preference, except that models with the highest numbers of iterations and a slow learning rate tended to perform worse.


#### RMSE and MAE Comparison



```{r}

# Add information about the optimal model based on R-squared
boosted_performance <- boosted_models3 %>% 
  pivot_longer(cols = c(RMSE, MAE), names_to = "Metric", values_to = "Performance") %>% 
  group_by(dataset, Metric) %>% 
  mutate(
    best = Performance == min(Performance, na.rm = T),
     transformed = if_else(str_detect(dataset, "log"), "Untransformed", "Transformed")
    ) %>% 
  ungroup() %>% 
  nest(.by = c(Metric, transformed)) 
    

# Create list for all plots
plots_boosted <- vector("list", nrow(boosted_performance))


# Creat heatmaps for each metric and separately for transformed and untrasformed datasets
for (i in 1:nrow(boosted_performance)) {
  
  plots_boosted[[i]] <-  boosted_performance$data[[i]] %>% 
    # filter(str_detect(dataset, "original"), Metric == "RMSE") %>% 
    ggplot(aes(factor(max_depth), eta_nround)) +
    geom_tile(aes(fill = Performance)) +
    geom_point(data = filter(boosted_performance$data[[i]], best), color = "red") +
    facet_wrap(~dataset, scales = "free") +
    labs(
     y = "Learning rate x boosting iterations",
     x = "Maximum depth of a tree",
      title = str_c(
        "Comparison of ", boosted_performance$transformed[[i]], " models in terms of ", boosted_performance$Metric[[i]]
        )
      ) +
    heatmap_aesthetics2
}


```



```{r}
# Compare models in terms of 
cowplot::plot_grid(plots_boosted[[2]], plots_boosted[[4]], nrow = 2)
```


```{r}
# Compare models in terms of
cowplot::plot_grid(plots_boosted[[1]], plots_boosted[[3]], nrow = 2)


```



### Predictors' importance

```{r}


# Plot importance of all predictors for the best model for each dataset type
best_boosted %>% 
  mutate(importance = map(fit, ~xgb.importance(model = .x))) %>% 
  select(-fit) %>% 
  unnest(importance) %>% 
  pivot_longer(-c(dataset, Feature), names_to = "type") %>% 
  ggplot(aes(y = Feature, x = value, color = dataset)) +
  geom_point() +
  facet_wrap(~type, scales = "free_x")
  

```

Judging by gain, number of Games had the highest contribution to the models. 
This was closely followed by the number of Games Started and Field Goals, perhaps with the exception of the model fitted to the full dataset with a log-transformed salary. The number of Assists, Personal Fouls, and, again with the exception of the full log-transformed dataset, Two-Point Goals were also relatively important.

Additionally, in the case of the full datasets, the importance of percentage metrics for Two-Point Goals, Field Goals, and Free Throws was on par with the importance of the number of Assists and Personal Fouls.



#### Partial Dependence Plots



```{r}
set.seed(123)




# Enable parallel computing
plan(multisession)


# Extract partial predictions for most important predictors
best_boosted2 <- best_boosted %>% 
  left_join(model_datasets) %>% 
  mutate(
    plots_Games = future_map2(fit, x_train, ~pdp::partial(.x, train = .y, pred.var = "Games")),
    plots_GS = future_map2(fit, x_train, ~pdp::partial(.x, train = .y, pred.var = "GS")),
    plots_FG = future_map2(fit, x_train, ~pdp::partial(.x, train = .y, pred.var = "FG")),
    plots_Two_P = future_map2(fit, x_train, ~pdp::partial(.x, train = .y, pred.var = "Two_P")),
    plots_PF = future_map2(fit, x_train, ~pdp::partial(.x, train = .y, pred.var = "PF")),
    plots_AST = future_map2(fit, x_train, ~pdp::partial(.x, train = .y, pred.var = "AST"))
  ) %>% 
  select(dataset, starts_with("plots")) %>% 
  pivot_longer(-dataset, names_to = "predictor_name", values_to = "partial_predictions") 

plan(sequential)
```


```{r}



best_boosted2 %>% 
  mutate(
    partial_predictions = map(partial_predictions, ~set_names(.x, nm = c("predictor", "salary")))
  ) %>% 
  unnest(partial_predictions) %>% 
  mutate(
    predictor_name = str_replace(predictor_name, "plots_", ""),
    salary = if_else(str_detect(dataset, "log"), exp(salary), salary)
    ) %>% 
  ggplot(aes(predictor, salary, color = dataset, group = dataset)) +
  geom_line() +
  facet_wrap(~predictor_name, scales = "free") +
  theme(legend.position = "top") +
  guides(color = guide_legend(nrow = 2))

```



The more Assists, Goals, Games Started, and Two-point goals there were, the higher the salary was, generally speaking. The optimal number of Field Goals, Games Started, and Two-point Goals was around 130, 30, and 90 respectively, with higher numbers not contributing noticeably to a bigger salary. The exception was the number of Field Goals for the model applied to the reduced dataset with the untransformed salary, where going beyond 250 goals was associated with a substantial surge in salary. On a similar note, the maximum salary was reached with a smaller number of Assists when models were applied to the full datasets (around 70) compared to models applied to the reduced datasets (around 180).

In contrast, the more Games and Personal Fouls there were, the smaller the salary. Salary dropped significantly with around 10 Personal Fouls and remained at a similar level until another drop occurred with more than 50 fouls, except in the model applied to the full dataset with the untransformed outcome, where the only drop occurred around 50 fouls. In the case of the number of Games, salary started consistently dropping after more than 10 games




### Summary





# Final models comparison

## Functions for performance metrics

```{r}

round_number <- 3

# Obtain Mean-squared error
rmse_own <- function(predicted, true_y) {
  
  
  # Obtain mse
  rmse <- sqrt(mean((true_y - predicted)^2))
  rmse <- round(rmse, round_number)
  
  return(rmse)
  
}


# Obtain Mean-median error
mae_own <- function(predicted, true_y) {
  
  # Obtain mse
  mae <- mean(abs(true_y - predicted))
  mae <- round(mae, round_number)
  
  return(mae)
  
}

# Obtain R-squared
rsquare_own <- function(predicted, true_y) {
  
  
  # Obtain mse
  r2 <- 1 - (sum((true_y - predicted) ^ 2) / sum((true_y - mean(true_y)) ^ 2))
  r2 <- round(r2, round_number)
  
  return(r2)
  
}


```



## Fit models onto test dataset

```{r}

# Remove unnecesary columns for best models merging
best_lasso_reduced <- best_lasso_reduced %>% 
  select(-test_dummy) %>% 
  mutate(fit_class = "lm lasso")

# Compare the best sets of models for all model types on the test dataset
best_tested <- best_boosted %>% 
  add_row(best_forest) %>% 
  add_row(best_tree) %>% 
  add_row(best_lm) %>% 
  mutate(fit_class = map_chr(fit, class)) %>% 
  add_row(best_lasso_reduced) %>% 
  left_join(model_datasets)  %>% 
  mutate(
    test = if_else(fit_class %in% c("lm", "rpart", "lm lasso"), test_dummy, x_test)
    ) %>% 
  mutate(
    predicted = map2(fit, test, predict),
    RMSE = map2_dbl(predicted, y_test, rmse_own),
    MAE = map2_dbl(predicted, y_test, mae_own),
    Rsquared = map2_dbl(predicted, y_test, rsquare_own),
    best = Rsquared == max(Rsquared)
  ) %>% 
  arrange_own()

# Extract only performance metrics
best_tested2 <- best_tested %>% 
  select(dataset, fit_class, RMSE, MAE, Rsquared, best)

# Show the model comparison in a table
best_tested2 %>% 
  select(-best) %>% 
  kable2()

```

Across all three model evaluation metrics, the best prediction was achieved with gradient-boosting decision trees models fitted to the datasets without the goal percentage metrics. 

Interestingly, when comparing models fitted to the datasets that also included the goal percentage metrics, linear models obtained using the stepwise selection methods resulted in the best prediction of salary.


```{r}

best_tested2 %>% 
  ggplot(aes(dataset, fit_class, fill = Rsquared)) +
  geom_tile() +
  geom_point(data = filter(best_tested2, best), color = "red") +
  heatmap_aesthetics

```

# Discussion


The WNBA salary prediction analysis explored multiple approaches to understand which performance statistics of WNBA players best predicted their salaries. 
The data included WNBA player stats from 2017 to 2023, alongside player salaries from one year later, adjusted for inflation using the Consumer Price Index (CPI). 

## Data Preparation and Preliminary Findings

1. **Data Characteristics and Challenges**:
   - **Multicollinearity**: The dataset suffered from multicollinearity, especially among performance metrics like field goals and field goal attempts, which made it difficult to separate individual effects. The correlation between such variables was extremely high (|cor| > 0.8).
   - **Methodological Issues**: Percentage variables for goal attempts posed a challenge, especially for players who had no attempts, requiring four different versions of the dataset to address these limitations.
   - **Cleaned Dataset**: The final dataset after cleaning consisted of `r nrow(model_datasets$whole[[1]])` observations. It had a right-skewed distribution for many performance variables, such as field goals, points, and rebounds, with most players having low to mid-range performance stats. This distribution pattern fits a Pareto distribution, common in human performance data.
   - **Player Sample Bias**: The analysis recognized a potential bias because salary information was more readily available for higher-performing players, limiting the generalizability of findings across all WNBA players.

2. **Model Comparisons**:
   Several supervised models were trained, including **linear regression (stepwise selection)**, **Lasso regression**, **decision tree**, **random forest**, and **XGBoost (gradient boosting)**. The **XGBoost model** fitted to the reduced dataset with untransformed salary data produced the best performance overall, closely followed by the same model fitted to the log-transformed reduced dataset.
   
   - **Best Models**: The **XGBoost boosted tree models** performed best, explaining around 32% of the variance in salary. These models utilized performance measures like **field goals**, **games started**, **assists**, and **two-point field goals** as the most important predictors, which were positively associated with salary. Conversely, **number of games** and **personal fouls** negatively affected salary.
   - **Tree Models**: Decision tree models reported to have the worst performance, regardless of the dataset, which might be explained by high correlations of predictors.
   - **Random Forest Models**: These performed better than decision trees, comparably to linear regression models, and worse than boosting models, strongly overlapping with the latter in terms of the most important predictors.
   - **Linear Models (Stepwise and Lasso)**: Like Random Forest models, these also achieved lower predictive power compared to boosted tree models. Stepwise models highlighted variables like **field goals**, **number of games**, and **year** as key predictors. Lasso regression models reduced multicollinearity but still performed slightly worse than tree-based approaches.

3. **Key Predictors of Salary**:
   - **Field Goals (FG)**: This was consistently the most important predictor across models. More field goals led to a higher salary, but the relationship leveled off after reaching around 130–150 goals.
   - **Games Started (GS)**: Players who started more games typically earned higher salaries, with the critical threshold for a noticeable salary increase being around 10–11 games started.
   - **Two-Point Field Goals (2P)**: Similar to field goals, two-point goals had a strong, positive association with salary.
   - **Personal Fouls (PF)**: Interestingly, more personal fouls were associated with lower salaries.
   - **Games Played (G)**: A higher number of games was negatively correlated with salary, which could indicate that players with higher workloads earned relatively less for their performance compared to more specialized or less frequent starters.

4. **Discussion of Model Limitations**:
   - **Model Limitations**: The analysis highlighted several limitations, such as **non-linearity** in the relationships between predictors and salaries, which made traditional linear models less effective. The **best-performing models** were those that could account for complex interactions, such as XGBoost.
   - **Heteroscedasticity and Multicollinearity**: The models showed signs of **heteroscedasticity** (i.e., the variance of errors was not constant across predictions), particularly in the untransformed models. Despite attempts to address this with log transformations, the issue persisted to some degree.
   - **External Validity**: The project acknowledged that due to incomplete salary data (missing for many lower-paid players), the analysis likely favored better-performing players, limiting the external validity of the conclusions for all WNBA players.

5. **Theoretical Considerations**

covid


## Conclusion

The project successfully predicted WNBA players' salaries with reasonable accuracy using advanced machine learning models like XGBoost and random forests. These models highlighted the complexity of the relationship between player performance and salary, showing that direct performance metrics (field goals, games started) were strong predictors of salary, while other metrics like the number of games and personal fouls had a negative impact.

The analysis suggested that non-linear models are better suited to capturing the intricate relationships between salary and player performance, particularly when addressing issues like multicollinearity and non-linearity.


# Discussion



-   the separation to four dataset did play an important role which was reflected in relatively big difference in performance metrics between final models
    -   it showed that including goal stats both in terms of percentage of successful attempts and pairs of absolute goal attempts and successes resulted in worse models
    -   this could be explained by its methodological limitations that had few consequences
        -   First, the percentage variables contained missing values for players who had both 0 attempts and 0 successes 
    -   this could be explained by either of the two reasons: 
        1) percentages of successful attempts had missing values making the datasets reduced of them more complete
        2) 
-   The role of COVID

Limitations
-   The salary data was not available for all players, especially those with lower wages



